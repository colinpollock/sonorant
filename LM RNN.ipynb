{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phoneme Language Model #\n",
    "This notebook builds a language model over English sounds (also known as [phonemes](https://en.wikipedia.org/wiki/Phoneme)). Its goal is to model what it means to be a valid English pronunciation. In this notebook I'll use it to:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.preprocessing import normalize\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sonorous.languagemodel import LanguageModel, ModelParams, Vocabulary\n",
    "from sonorous.pronunciationdata import load_pronunciations\n",
    "from sonorous.utils import split_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data ##\n",
    "The data for this model comes from the [CMU Pronouncing Dictionary](http://www.speech.cs.cmu.edu/cgi-bin/cmudict), which contains over one hundred thousand pronunciations. Each pronuncation is in [ARPABET](https://en.wikipedia.org/wiki/ARPABET), a set of symbols for representing English speech sounds. In ARPABET the word \"fish\" is represented by the sequence of phonemes /F IH1 SH/. You can probably guess the first and third sounds. The vowel in the middle has \"1\" at the end to indicate it has the primary stress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll use the `load_pronunciations` function to load the Pronouncing Dictionary into a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 124,996 pronunciations\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pronunciation</th>\n",
       "      <th>pronunciation_string</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>folly</th>\n",
       "      <td>(F, AA1, L, IY0)</td>\n",
       "      <td>F AA1 L IY0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dehnert</th>\n",
       "      <td>(D, EH1, N, ER0, T)</td>\n",
       "      <td>D EH1 N ER0 T</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aloha</th>\n",
       "      <td>(AH0, L, OW1, HH, AA0)</td>\n",
       "      <td>AH0 L OW1 HH AA0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>benackova</th>\n",
       "      <td>(B, EH2, N, AH0, K, OW1, V, AH0)</td>\n",
       "      <td>B EH2 N AH0 K OW1 V AH0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>geurin</th>\n",
       "      <td>(ZH, ER0, AE1, N)</td>\n",
       "      <td>ZH ER0 AE1 N</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              pronunciation     pronunciation_string  length\n",
       "word                                                                        \n",
       "folly                      (F, AA1, L, IY0)              F AA1 L IY0       4\n",
       "dehnert                 (D, EH1, N, ER0, T)            D EH1 N ER0 T       5\n",
       "aloha                (AH0, L, OW1, HH, AA0)         AH0 L OW1 HH AA0       5\n",
       "benackova  (B, EH2, N, AH0, K, OW1, V, AH0)  B EH2 N AH0 K OW1 V AH0       8\n",
       "geurin                    (ZH, ER0, AE1, N)             ZH ER0 AE1 N       4"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pronunciations = load_pronunciations()\n",
    "print(f\"There are {len(pronunciations):,} pronunciations\")\n",
    "\n",
    "pronunciations.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the pronunciation for \"fish\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'F IH1 SH'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pronunciations.loc['fish', 'pronunciation_string']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here are all of the pronunciations for the word \"tomato\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pronunciation_string</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tomato</th>\n",
       "      <td>T AH0 M EY1 T OW2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tomato</th>\n",
       "      <td>T AH0 M AA1 T OW2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       pronunciation_string\n",
       "word                       \n",
       "tomato    T AH0 M EY1 T OW2\n",
       "tomato    T AH0 M AA1 T OW2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pronunciations.loc['tomato', ['pronunciation_string']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 73 phonemes in the vocabulary\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocabulary.from_texts(pronunciations.pronunciation.values)\n",
    "print(f\"There are {len(vocab)} phonemes in the vocabulary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = ModelParams(\n",
    "    rnn_type='gru', embedding_dimension=50, hidden_dimension=1, num_layers=1,\n",
    "    max_epochs=3, early_stopping_rounds=3\n",
    ")\n",
    "\n",
    "language_model = LanguageModel(vocab, model_params, 'cpu')\n",
    "language_model.fit(train_df.pronunciation.values.tolist(), dev_df.pronunciation.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_model.fit(train_df.pronunciation.values.tolist(), dev_df.pronunciation.values.tolist(), max_epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_model.fit(train_df.pronunciation.values.tolist(), dev_df.pronunciation.values.tolist(), max_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_model.fit(train_df.pronunciation.values.tolist(), dev_df.pronunciation.values.tolist(), max_epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_model.fit(train_df.pronunciation.values.tolist(), dev_df.pronunciation.values.tolist(), max_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_model.fit(train_df.pronunciation.values.tolist(), dev_df.pronunciation.values.tolist(), max_epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model ##\n",
    "The module `languagemodel.py` contains a class `LanguageModel` that implements a simple neural language model. It's a fairly simple PyTorch neural network comprised of the following layers:\n",
    "1. Embedding layer to translate each phoneme into a dense vector.\n",
    "2. An recurrent neural network (RNN) layer that processes each input phoneme sequentially and for each step generates (a) a hidden representation to pass on to the next step and (b) an output.\n",
    "3. A linear layer that decodes the outputes (2b) into distributions over each phoneme."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go through a simple example of what happens when we pass the pronunciation /F IH1 SH/ through the model. Ultimately what I want ouf of the model is a prediction at each position of what the next phoneme should be. For example, when a well trained model is processing \"F\" I would hope it assigns a high probability to \"IH1\" coming next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First I'll define our model. Note that I'm not actually fitting the model to any data so the output will be random. The parameters aren't optimal, but again that doesn't matter here since I just want to show the flow of data through the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: comment on this\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocabulary.from_texts(df.pronunciation.values)\n",
    "\n",
    "model_params = ModelParams(\n",
    "    rnn_type='rnn', embedding_dimension=10, hidden_dimension=3, num_layers=1,\n",
    "    max_epochs=3, early_stopping_rounds=3\n",
    ")\n",
    "\n",
    "language_model = LanguageModel(vocab, model_params, 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next I'll define the pronuncation we'll be working with. This is the standard pronunciation of \"fish\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pronunciation = (\"F\", \"IH1\", \"SH\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within the neural network, phonemes are represented as integer indices. So the phoneme \"F\" might correspond go 23. Overall there are 73 distinct phonemes. The class `Vocabulary` finds every distinct phoneme in the pronunciations and constructs a dictionary mapping each one to an integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'There are {len(vocab)} distinct phonemes.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before passing the pronunciation into the neural network I need to add dummy tokens to the pronunciation indicating its start and end. This allows the model to learn transition probabilities from the start of the word to the first phoneme, and from the last phoneme to the end of the word. We end up with this: `[\"START\", \"F\", \"IH1\", \"SH\", \"END\"]`.\n",
    "\n",
    "I also need to translate each phoneme into an integer index within the model. The `encode_pronunciation` function handles both of these tasks. It uses the `phoneme_to_idx` mapping I built above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = vocab.encode_text(pronunciation)\n",
    "input_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is what each of those ints maps to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in input_:\n",
    "    phoneme = vocab.token_from_idx(idx)\n",
    "    print(f'{idx} => {phoneme}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we're ready to pass the input into the model's `forward` function. You can see the full function in `sonorous/languagemodel.py: LanguageModel`, but I'll walk through it layer by layer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Torch, the `forward` function is the forward pass through the network, taking in inputs and outputting predictions. This model's `forward` function expects a Tensor of dimension (number of batches, number of steps, vocabulary size).\n",
    "\n",
    "In this case I have a single pronunciation so the batch size is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Input's shape:\", input_.shape)\n",
    "batch_input = torch.LongTensor(input_).unsqueeze(0)\n",
    "print(\"Batch input's shape:\", batch_input.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing the `forward` function does is embed each phoneme using an [nn.Embedding](https://pytorch.org/docs/stable/nn.html#embedding). Each phoneme has a dedicated embedding vector of length `embedding_dimension`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded = language_model._encoder(batch_input)\n",
    "print(embedded.shape)\n",
    "embedded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entire embedding is of the shape (meow), with M representing the number of phonemes in the input (including the dummy ones) and N repreesnting the embedding dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Side Note*: I want to talk a little bit about why embeddings are useful and whether they make sense here.\n",
    "\n",
    "For this language model the vocabulary is the set of possible phonemes, of which there are MEOW. In most natural language processing tasks the tokens in the vocabulary are words (e.g. \"cat\", \"threw\"), where there are likely to be tens of thousands. So when embedding words using an embedding dimension of 100, each word is represented by a dense vector of 100 floats. Without embeddings you would need to represent each of your words with a one hot vector the size of your entire vocabulary. So embeddings result in fewer model parameters since the model only needs to know about those 100 floats. Since embeddings are updated during backpropagation, the model learns how to represent each token as a vector.\n",
    "\n",
    "For this phoneme language model the vocabulary size is already very small to begin with and I initially was going to just represent each phoneme with a one hot vector. However, I wanted to use embeddings because I was hoping that the model would learn to represent each phoneme as its component parts. For example, the embedding space could capture whether a phoneme is a vowel or a consonant, whether it's [voiced or voiceless](https://en.wikipedia.org/wiki/Voice_(phonetics)), how [sonorant](https://en.wikipedia.org/wiki/Sonorant) it is, etc. Some limited probing of the embeddings generated didn't turn up anything too interesting (see more below) so I'm not sure how much they're helping, if at all. I have not measured model performance just using one hot encoding, but I wouldn't be surprised if it were equal or better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have the output of our embedding layer. We can pass this through the RNN layer, resulting in `rnn_output` and `hidden_state`. I won't go into detail on how RNNs work, but the basic idea is it has shared weights that are applied sequentially to every token (i.e. step) in the input. At each step an output and a hidden state are produced. The hidden state can be passed on to the next step, and the output can be used to make a prediction.\n",
    "\n",
    "The `rnn` layer below operates on the full sequence, so the results are for the entire sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_output, hidden_state = language_model._rnn(embedded)\n",
    "print(rnn_output.shape)\n",
    "rnn_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our application we can ignore the `hidden_state`-- the `rnn_output` is the interesting part. The shape of `rnn_output` is MEOW MEOW MEOW. The first dimension is for the batch, and we only have a single input in our batch. The second dimension is for each of the input phonemes: `input_` was of length MEOW so this is too. The third dimension corresponds to `hidden_dimension`: you can think of this as the state of the RNN at each step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're getting to the interesting part. As I said at the beginning of this section, I want the output of the RNN at each position to be predictions for the *next* position. So I'll apply a linear layer to the `rnn_output`, resulting in a vector the size of the vocabularly at each position. The [softmax](https://pytorch.org/docs/stable/nn.functional.html#softmax) function normalizes the outputs into probability distributions for each prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = language_model._decoder(rnn_output)\n",
    "probabilities = F.softmax(outputs, dim=-1).squeeze()\n",
    "probabilities.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of `probabilities` is MEOW X MEOW: for each of the MEOW phonemes in the input there's a distribution over all phonemes indicating what the model thinks the next phoneme in the input should be. The first phoneme in the input is the `START` token, so let's see what the model thinks should come next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities_for_first_phoneme = probabilities[0]\n",
    "most_likely_first_phoneme_idx = probabilities_for_first_phoneme.argmax().item()\n",
    "most_likely_first_phoneme = vocab.token_from_idx(most_likely_first_phoneme_idx)\n",
    "most_likely_first_phoneme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the model predicts /HH/ to be the first phoneme in the word. Since the model isn't fit yet this is just a random guess."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MEOW update based on what actually comes out\n",
    "\n",
    "Note that this isn't a very good prediction. If you look at the rest of the distribution you'll see it doesn't make sense. In order to get the model to make good predictions (e.g. that \"S\" is likely as the first phoneme in a word) we'll need to first train a good model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the Best Model ##\n",
    "In this section I'll train a number of models on the train set and select the one that has the lowest error on the dev set. I'll split the DataFrame of pronunciations into three DataFrames, with 79% for training, 20% for dev/validation, and 1% for testing of the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(98746, 25000, 1250)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pronunciations, dev_pronunciations, test_pronunciations = split_data(pronunciations, dev_proportion=.2, test_proportion=.01)\n",
    "len(train_pronunciations), len(dev_pronunciations), len(test_pronunciations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that I ran a larger parameter search before and saw that GRUs were consistently outperforming LSTMs and vanilla RNNs. In the search below I'm looking for the model with the lowest dev error. There are 12 (4 * 3) models to build, and for each one I'm measuring train and dev error at every epoch. So if each model trains for the maximum of 2,000 epochs I would end up with 12 * 2,000 = 24,000 models to choose from. There's a good chance I'm overfitting the dev set with such a large search, but I'll inspect the learning curves to try to avoid selecting an iteration that randomly did well.\n",
    "\n",
    "While each model trains for a maximum of 2000 epochs, it stops early if the dev error does not decrease for three epochs in a row. Since I'm going to be selecting the model with the lowest dev error there's no reason to keep training a model once it's started overfitting. Alternatively I could train all models to convergence and then add regularization to reduce the complexity and identify the sweet spot, but that's far more time consuming because it requires training more models and each of them for longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5df4aac550b4d098bbe8c482dec4e2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Params: ModelParams(rnn_type='rnn', embedding_dimension=50, hidden_dimension=2, num_layers=1, max_epochs=1000, early_stopping_rounds=1, learning_rate=0.001, dropout=0, l2_strength=0, batch_size=1024)\n",
      "Epoch 1: train loss: 2.7358\tdev loss: 2.9814\n",
      "\tGenerated: in train: 3%, assess: 0%, novel: 97%\n",
      "\t AE1 AY2 AH0 AA2 UH1\n",
      "\t N ZH UW1 OY2\n",
      "\t \n",
      "\t AE1 IY1 UH1 T IH1\n",
      "\t OY2 IY1 TH UW2 N UW0 UW1 AE0 AH2 AA1 IY0 AO2 F UH1 AY1\n",
      "Epoch 2: train loss: 2.3473\tdev loss: 2.6413\n",
      "\tGenerated: in train: 4%, assess: 0%, novel: 96%\n",
      "\t P UW1 IH2 AW0 AW1 L IH0 EH0 ER0 UH1 AE1 CH P JH ER2 UW G AH2 AH2 HH G UH1 IH0\n",
      "\t UW2 TH S OW2 IH2\n",
      "\t TH IH1 AA0 EH1 IH2 AH1 DH W\n",
      "\t R UH1 R AW2 OW0 L OW0 AE1\n",
      "\t EY0 R DH EY2 OY1 UW2 AW1\n",
      "Epoch 3: train loss: 2.0126\tdev loss: 2.3506\n",
      "\tGenerated: in train: 7%, assess: 0%, novel: 93%\n",
      "\t UW2 UH1 EH2 AY1 EH1 UW1 EH0\n",
      "\t OW0 CH SH AO0 EY0 N W EY1 UW W CH\n",
      "\t P OY2\n",
      "\t ER0 AH0 F K AO1 IY1 K\n",
      "\t Y AE0\n",
      "Epoch 4: train loss: 1.7399\tdev loss: 2.1130\n",
      "\tGenerated: in train: 8%, assess: 0%, novel: 92%\n",
      "\t \n",
      "\t IY1\n",
      "\t IH1 AW1\n",
      "\t EH1 OY0 V N G IY1 CH\n",
      "\t Y AH2\n",
      "Epoch 5: train loss: 1.5286\tdev loss: 1.9271\n",
      "\tGenerated: in train: 4%, assess: 1%, novel: 95%\n",
      "\t \n",
      "\t IY1 UH1 F AH0 EY1 AY1 UH1 F OW2 IY1 R AH2 OW1\n",
      "\t EY2 OY2\n",
      "\t K DH OY2 AA2\n",
      "\t AA1 UH1 V Y AH1 EY2\n",
      "Epoch 6: train loss: 1.3698\tdev loss: 1.7844\n",
      "\tGenerated: in train: 6%, assess: 0%, novel: 94%\n",
      "\t \n",
      "\t IH0 AE1 R EH0 AH0 N OY2 AA0 B EH1 Y\n",
      "\t ER0 OY2 IH2 K IY0\n",
      "\t OW2 AA2 UW0\n",
      "\t AE0 JH OY0 K EY0\n",
      "Epoch 7: train loss: 1.2514\tdev loss: 1.6757\n",
      "\tGenerated: in train: 7%, assess: 0%, novel: 93%\n",
      "\t ER2 K IY0 AE1 IY0 JH\n",
      "\t AY2 UW0 K AO2\n",
      "\t T AY2 HH EH1\n",
      "\t AO1 AE1 L UH0 UH1 OY2 AA2 IH0\n",
      "\t EY1 DH M EY2\n",
      "Epoch 8: train loss: 1.1620\tdev loss: 1.5913\n",
      "\tGenerated: in train: 1%, assess: 0%, novel: 99%\n",
      "\t Z OY2\n",
      "\t NG UW2 DH JH\n",
      "\t L UW2 IH2 F IY1 NG AA2 AY0 K AW1 OW0\n",
      "\t ER1 NG AA0\n",
      "\t OY2 AA2 B K AY2 OW2 IH2\n",
      "Epoch 9: train loss: 1.0940\tdev loss: 1.5254\n",
      "\tGenerated: in train: 1%, assess: 0%, novel: 99%\n",
      "\t AA2 AE1 Z\n",
      "\t M B ER1 OY0 EY1 OY2 DH B S\n",
      "\t M IH1 IH0 ER0 AE2\n",
      "\t AH0 G UH1 OY2 T IY1 EH1 UH1 ER0 K L\n",
      "\t UW N N AH2 ER2 Y NG\n",
      "Epoch 10: train loss: 1.0406\tdev loss: 1.4720\n",
      "\tGenerated: in train: 4%, assess: 0%, novel: 96%\n",
      "\t AE0\n",
      "\t G CH\n",
      "\t L Y OW0\n",
      "\t DH CH AY1 IH0 B AY1 EH1\n",
      "\t OW2 S DH\n",
      "Epoch 11: train loss: 0.9979\tdev loss: 1.4283\n",
      "\tGenerated: in train: 4%, assess: 0%, novel: 96%\n",
      "\t L N\n",
      "\t OW1 N IY0 EH2 T AH2 DH OW1 S IY2 T M AY2 EY0 IY0 DH AH2\n",
      "\t IH1\n",
      "\t K JH AY1 AO2 IH2 UW0 K AH0 IH0 EY2\n",
      "\t AO0 UH1 L IH0 B G UH1 AH0 ZH L IH0 AH0 NG AH0 CH R Z\n",
      "Epoch 12: train loss: 0.9635\tdev loss: 1.3924\n",
      "\tGenerated: in train: 8%, assess: 0%, novel: 92%\n",
      "\t M AY1 L S AH0\n",
      "\t EH0 IY0 AH0 CH SH N\n",
      "\t UW2 N R\n",
      "\t \n",
      "\t AE0 AH2 L EH1\n",
      "Epoch 13: train loss: 0.9347\tdev loss: 1.3618\n",
      "\tGenerated: in train: 7%, assess: 0%, novel: 93%\n",
      "\t R\n",
      "\t UW1\n",
      "\t T\n",
      "\t ER2 AH0 Y EH2 L AH0\n",
      "\t AH0 Z UW0 F AA2 AO1 OW0 N\n",
      "Epoch 14: train loss: 0.9114\tdev loss: 1.3364\n",
      "\tGenerated: in train: 10%, assess: 0%, novel: 90%\n",
      "\t AH0 EY1 L P M\n",
      "\t AY1 EY1 K T M AW1\n",
      "\t OW0 S K AH2 UW0 R L UH1 D N N L UH1 K\n",
      "\t K\n",
      "\t \n",
      "Epoch 15: train loss: 0.8909\tdev loss: 1.3140\n",
      "\tGenerated: in train: 4%, assess: 0%, novel: 96%\n",
      "\t NG K ER0 T\n",
      "\t AW1 AW0\n",
      "\t L EY2 AO1 R\n",
      "\t Z G ER0 AH0 Z S K R\n",
      "\t L\n",
      "Epoch 16: train loss: 0.8735\tdev loss: 1.2946\n",
      "\tGenerated: in train: 4%, assess: 0%, novel: 96%\n",
      "\t AA1\n",
      "\t IH1 AH0\n",
      "\t K K L T B ER1 AH0\n",
      "\t ER0\n",
      "\t L\n",
      "Epoch 17: train loss: 0.8591\tdev loss: 1.2784\n",
      "\tGenerated: in train: 10%, assess: 0%, novel: 90%\n",
      "\t L SH AA1\n",
      "\t M\n",
      "\t AA1 UW0 T R EH0 OW1 UH1 S AY2 OW2 P L UW1 T\n",
      "\t B T EH2 ER2 OY2 G\n",
      "\t S AA0\n",
      "Epoch 18: train loss: 0.8455\tdev loss: 1.2627\n",
      "\tGenerated: in train: 5%, assess: 0%, novel: 95%\n",
      "\t AH0 P AH0 R\n",
      "\t ER0 IY1\n",
      "\t V K\n",
      "\t T CH\n",
      "\t IH0 EH1 N\n",
      "Epoch 19: train loss: 0.8335\tdev loss: 1.2489\n",
      "\tGenerated: in train: 4%, assess: 1%, novel: 95%\n",
      "\t T AH2 IH0 M\n",
      "\t N L L\n",
      "\t AH0 Z\n",
      "\t UW0 K Z CH B OW2 AH0 R T AY1 AW1 B M\n",
      "\t W ER2 L OY2 Z\n",
      "Epoch 20: train loss: 0.8239\tdev loss: 1.2378\n",
      "\tGenerated: in train: 3%, assess: 0%, novel: 97%\n",
      "\t L G\n",
      "\t M S CH\n",
      "\t M B DH F UW1 K OY2\n",
      "\t OW1 AH0 AE1 AH0 HH UW2\n",
      "\t HH ZH P R Z\n",
      "Epoch 21: train loss: 0.8151\tdev loss: 1.2275\n",
      "\tGenerated: in train: 7%, assess: 0%, novel: 93%\n",
      "\t AO1 OW2 ER0 AH0 NG L EY2 AH0 N Z AO1 HH OW1 M ER1 L W UW\n",
      "\t M\n",
      "\t IY0\n",
      "\t K TH\n",
      "\t CH P AH0 AH0 OW0 N EY2\n",
      "Epoch 22: train loss: 0.8071\tdev loss: 1.2183\n",
      "\tGenerated: in train: 6%, assess: 0%, novel: 94%\n",
      "\t AH0 W AH1 IY1 P UH1 K AW2 IH0 EH1 AH0 AA2 HH IH2 UW2 EY2\n",
      "\t T L\n",
      "\t UH1 S OW2 AH0 L T AH2 R\n",
      "\t ER2 L OY2 EY1 IH0 L L L IH0 AH0 HH T N\n",
      "\t AY1 D\n",
      "Epoch 23: train loss: 0.7973\tdev loss: 1.2059\n",
      "\tGenerated: in train: 4%, assess: 0%, novel: 96%\n",
      "\t N AO1 OY2 AH1 L EH1 IH0 K T\n",
      "\t M IY0 G\n",
      "\t Y\n",
      "\t AH0 P UW2\n",
      "\t R P UW1\n",
      "Epoch 24: train loss: 0.7907\tdev loss: 1.1980\n",
      "\tGenerated: in train: 3%, assess: 0%, novel: 97%\n",
      "\t G IY1 P\n",
      "\t T\n",
      "\t HH AA0 Z\n",
      "\t HH AE2 D D UW1 B EH1 EY1 AY0 K T AH0 R R AH2 R\n",
      "\t N NG IY0 EH1 AE1 N B AH1 V HH AA2\n",
      "Epoch 25: train loss: 0.7847\tdev loss: 1.1908\n",
      "\tGenerated: in train: 5%, assess: 0%, novel: 95%\n",
      "\t N\n",
      "\t S UW2 N\n",
      "\t P T G T AA0 P N T N\n",
      "\t AH0 K L N\n",
      "\t T\n",
      "Epoch 26: train loss: 0.7798\tdev loss: 1.1848\n",
      "\tGenerated: in train: 3%, assess: 0%, novel: 97%\n",
      "\t B R\n",
      "\t T Z\n",
      "\t M T V Z N\n",
      "\t D T\n",
      "\t M IY0 T OY2\n",
      "Epoch 27: train loss: 0.7742\tdev loss: 1.1779\n",
      "\tGenerated: in train: 8%, assess: 0%, novel: 92%\n",
      "\t S IY0 W\n",
      "\t N K AH0 IY0\n",
      "\t Y R N AH0 G D N D K\n",
      "\t N L OW0\n",
      "\t M ER2 CH AH2 T IH0 F\n",
      "Epoch 28: train loss: 0.7694\tdev loss: 1.1719\n",
      "\tGenerated: in train: 2%, assess: 0%, novel: 98%\n",
      "\t R ER0\n",
      "\t L\n",
      "\t AH1 T L\n",
      "\t B F AO1 K Z Z ER0\n",
      "\t M IH1 AH0 R EH1 P T K AY1 T L T T\n",
      "Epoch 29: train loss: 0.7611\tdev loss: 1.1604\n",
      "\tGenerated: in train: 7%, assess: 0%, novel: 93%\n",
      "\t UW0 P EH1 M\n",
      "\t ER2 N K AA1 Z S K D UW0 OW1 N R R L\n",
      "\t Z\n",
      "\t M AH0 NG IH1 D EY2 S IY0 Z IY0 IY0\n",
      "\t M\n",
      "Epoch 30: train loss: 0.7573\tdev loss: 1.1556\n",
      "\tGenerated: in train: 3%, assess: 0%, novel: 97%\n",
      "\t UH2 D IH0 L D\n",
      "\t L K EH1 L AO0 K IH0 S ER0 EH2 AH0 L IY1 M\n",
      "\t R T T\n",
      "\t L B G AH0 K IH0 EH1 IY1 K D V\n",
      "\t SH B F IY0\n",
      "Epoch 31: train loss: 0.7536\tdev loss: 1.1510\n",
      "\tGenerated: in train: 4%, assess: 0%, novel: 96%\n",
      "\t P L S UH1 K\n",
      "\t L T AH0 L IH0 D B HH L L S Z Z\n",
      "\t S\n",
      "\t NG AE1 EY2\n",
      "\t B N\n",
      "Epoch 32: train loss: 0.7495\tdev loss: 1.1456\n",
      "\tGenerated: in train: 1%, assess: 0%, novel: 99%\n",
      "\t N\n",
      "\t S AA2 EH2 T IH0 IH1 EY1 T B L N\n",
      "\t EH0 K\n",
      "\t N AH0 N AA2 NG EH1 L AH0\n",
      "\t Z IY0\n",
      "Epoch 33: train loss: 0.7461\tdev loss: 1.1413\n",
      "\tGenerated: in train: 4%, assess: 0%, novel: 96%\n",
      "\t S L G R AH0 AY1 S\n",
      "\t S M L ER0 IH0 L AW2\n",
      "\t N IH0 S Z EY0\n",
      "\t K S\n",
      "\t K N IH0 S T R\n",
      "Epoch 34: train loss: 0.7431\tdev loss: 1.1375\n",
      "\tGenerated: in train: 4%, assess: 1%, novel: 95%\n",
      "\t AE0 M\n",
      "\t P AE1 EY2\n",
      "\t M AY1 N\n",
      "\t T AO1 N\n",
      "\t T IY0\n",
      "Epoch 35: train loss: 0.7407\tdev loss: 1.1343\n",
      "\tGenerated: in train: 3%, assess: 0%, novel: 97%\n",
      "\t K AY2\n",
      "\t T L OY1 HH AY0 D AH0 T\n",
      "\t N IY1 T AA1 M IH1 R L L Y\n",
      "\t AH0 K AE2 F NG AY2 T\n",
      "\t \n",
      "Epoch 36: train loss: 0.7380\tdev loss: 1.1310\n",
      "\tGenerated: in train: 11%, assess: 0%, novel: 89%\n",
      "\t T AA1 IY1 K S OW1 K IH0 G EY1 B IH2 S R W T AW1 HH OY1 K AW1\n",
      "\t G R M L\n",
      "\t R\n",
      "\t AA1 K S\n",
      "\t D AH0 L IH1 N ER0 IY0 OW1 SH\n",
      "Epoch 37: train loss: 0.7361\tdev loss: 1.1284\n",
      "\tGenerated: in train: 7%, assess: 0%, novel: 93%\n",
      "\t Y OW1 P ER0 T IY0 ER0\n",
      "\t \n",
      "\t G R AH1 N Z\n",
      "\t AH0 K\n",
      "\t S T\n",
      "Epoch 38: train loss: 0.7334\tdev loss: 1.1249\n",
      "\tGenerated: in train: 6%, assess: 0%, novel: 94%\n",
      "\t S IH0 R\n",
      "\t N\n",
      "\t N V\n",
      "\t N L IH0 K M AY1 L IY0 IY0\n",
      "\t R ER0 AA2 N F D\n",
      "Epoch 39: train loss: 0.7316\tdev loss: 1.1226\n",
      "\tGenerated: in train: 2%, assess: 0%, novel: 98%\n",
      "\t IY1 K AH0 S M K\n",
      "\t AH0 N AO0 R ER0 IY0 V IY0 Y AA1 G AW0\n",
      "\t IH0 IH0 B R S\n",
      "\t \n",
      "\t S\n",
      "Epoch 40: train loss: 0.7294\tdev loss: 1.1198\n",
      "\tGenerated: in train: 1%, assess: 1%, novel: 98%\n",
      "\t M IY1 AA2 K R HH IY1 AE1 AH0 N IH0 R T\n",
      "\t N AE1 N S S\n",
      "\t M\n",
      "\t M ER1 T ER0\n",
      "\t N\n",
      "Epoch 41: train loss: 0.7279\tdev loss: 1.1178\n",
      "\tGenerated: in train: 5%, assess: 0%, novel: 95%\n",
      "\t F R N\n",
      "\t DH G N M EY1 K\n",
      "\t EH1 R\n",
      "\t IH1 S B OW1 L\n",
      "\t N UW B S AO1 SH AH0 L M OW1 K Z\n",
      "Epoch 42: train loss: 0.7259\tdev loss: 1.1153\n",
      "\tGenerated: in train: 3%, assess: 2%, novel: 95%\n",
      "\t N\n",
      "\t AA1 OW1 AH1 S ER2 L\n",
      "\t S V\n",
      "\t N D\n",
      "\t SH R IH0 AY1 Z\n",
      "Epoch 43: train loss: 0.7239\tdev loss: 1.1123\n",
      "\tGenerated: in train: 6%, assess: 1%, novel: 93%\n",
      "\t K K OY0\n",
      "\t M AA1 D AO1 M AW1 AY1 D F AE1 N T\n",
      "\t N V M AY0 G R\n",
      "\t K OY0 S AA2 M IY0 T\n",
      "\t S N\n",
      "Epoch 44: train loss: 0.7212\tdev loss: 1.1084\n",
      "\tGenerated: in train: 6%, assess: 0%, novel: 94%\n",
      "\t AH0 D AE1 N OW0 UH2 AH0 R\n",
      "\t S D EH1 AE1 L AH0 UW1\n",
      "\t L UW1 IH0 S HH\n",
      "\t K\n",
      "\t M N\n",
      "Epoch 45: train loss: 0.7186\tdev loss: 1.1051\n",
      "\tGenerated: in train: 10%, assess: 0%, novel: 90%\n",
      "\t L T\n",
      "\t SH\n",
      "\t AO1 F ER1 T AH1 D IH0 S\n",
      "\t R AH0 N AH0 S D AH0 R OY2 Z AH0 P L Y P Z\n",
      "\t S K\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46: train loss: 0.7150\tdev loss: 1.0994\n",
      "\tGenerated: in train: 8%, assess: 1%, novel: 91%\n",
      "\t AY1 S AE1 N\n",
      "\t EY1 T EH1 N\n",
      "\t N R\n",
      "\t L EH1 AH0 L V UW1\n",
      "\t M AH0 AH0 S\n",
      "Epoch 47: train loss: 0.7125\tdev loss: 1.0959\n",
      "\tGenerated: in train: 10%, assess: 0%, novel: 90%\n",
      "\t IH0 L\n",
      "\t AE1 L N N EY2 F L T OY1 R D\n",
      "\t D D\n",
      "\t M ER0 AA1 EH1 S T AH0 UH2 ER0\n",
      "\t M IY0 S OW1 R R ER0\n",
      "Epoch 48: train loss: 0.7096\tdev loss: 1.0920\n",
      "\tGenerated: in train: 3%, assess: 1%, novel: 96%\n",
      "\t ER1 B ER1 D OW1 D N D AH0 N\n",
      "\t S S R IY0\n",
      "\t B IY0 IY0 AH0 SH T\n",
      "\t S S\n",
      "\t N M AH0 N EH1 L B AY1 D\n",
      "Epoch 49: train loss: 0.7072\tdev loss: 1.0886\n",
      "\tGenerated: in train: 5%, assess: 1%, novel: 94%\n",
      "\t TH AH0 AE1 L OW1 M UW IY0\n",
      "\t R IY0\n",
      "\t AH0 G IH0 P K AY2 AH0 K D T\n",
      "\t K R B\n",
      "\t B AY2 K\n",
      "Epoch 50: train loss: 0.7041\tdev loss: 1.0837\n",
      "\tGenerated: in train: 9%, assess: 0%, novel: 91%\n",
      "\t IY0 L UH1 P\n",
      "\t L\n",
      "\t L N T S\n",
      "\t AH1 NG AH0 JH\n",
      "\t B S N\n",
      "Epoch 51: train loss: 0.7016\tdev loss: 1.0802\n",
      "\tGenerated: in train: 5%, assess: 0%, novel: 95%\n",
      "\t ER2 B IH2 T OW1 N\n",
      "\t UW2 P D AO1 P IH0 L L\n",
      "\t R Y OW1 K N AO1 N\n",
      "\t JH AO1 T\n",
      "\t EH1 M\n",
      "Epoch 52: train loss: 0.6998\tdev loss: 1.0778\n",
      "\tGenerated: in train: 9%, assess: 0%, novel: 91%\n",
      "\t K AY1 S V IY1 K\n",
      "\t K R\n",
      "\t L AE1 D T L D AH0 AA1 L IY1 AW1 AE1 L D\n",
      "\t \n",
      "\t \n",
      "Epoch 53: train loss: 0.6983\tdev loss: 1.0758\n",
      "\tGenerated: in train: 9%, assess: 0%, novel: 91%\n",
      "\t N ER1 G OW0 AH0 M AH0 R\n",
      "\t AE1 N\n",
      "\t Z AH0 Z\n",
      "\t B OW1 G\n",
      "\t L\n",
      "Epoch 54: train loss: 0.6972\tdev loss: 1.0741\n",
      "\tGenerated: in train: 10%, assess: 2%, novel: 88%\n",
      "\t TH S M\n",
      "\t AA1 N T P\n",
      "\t L AY1 S\n",
      "\t T CH\n",
      "\t K D P\n",
      "Epoch 55: train loss: 0.6960\tdev loss: 1.0725\n",
      "\tGenerated: in train: 12%, assess: 1%, novel: 87%\n",
      "\t F AH1 HH ER0 AA1 D\n",
      "\t UW0\n",
      "\t M IH0 AH1 T Z\n",
      "\t K AE1 AH0 D T\n",
      "\t HH UW\n",
      "Epoch 56: train loss: 0.6943\tdev loss: 1.0704\n",
      "\tGenerated: in train: 4%, assess: 2%, novel: 94%\n",
      "\t G S S Z\n",
      "\t G L OW0 OW0\n",
      "\t AH1 K B AY1 S\n",
      "\t EY1 P IH2 L\n",
      "\t IH0 EH0 L AA1 IH0 R IY0 ER0\n",
      "Epoch 57: train loss: 0.6929\tdev loss: 1.0682\n",
      "\tGenerated: in train: 4%, assess: 0%, novel: 96%\n",
      "\t S Z\n",
      "\t N K\n",
      "\t F AH0 D\n",
      "\t EH1 TH D D G\n",
      "\t T B ER1 EY2 F AA1\n",
      "Epoch 58: train loss: 0.6903\tdev loss: 1.0645\n",
      "\tGenerated: in train: 9%, assess: 0%, novel: 91%\n",
      "\t R AA1 B IH0 L R AY1 AE2 P N IH0 N NG\n",
      "\t P IH2 N\n",
      "\t EH1 P AH0 N T AE1 N EY1 T HH R OY1 SH\n",
      "\t K S L R IH1 HH AO1 T G UW1 R\n",
      "\t AE1 M\n",
      "Epoch 59: train loss: 0.6893\tdev loss: 1.0631\n",
      "\tGenerated: in train: 10%, assess: 0%, novel: 90%\n",
      "\t S\n",
      "\t IH1 N\n",
      "\t P IY0 L\n",
      "\t EH1 IY0\n",
      "\t K Y B OW2 N IY0\n",
      "Epoch 60: train loss: 0.6885\tdev loss: 1.0618\n",
      "\tGenerated: in train: 9%, assess: 2%, novel: 89%\n",
      "\t N T\n",
      "\t AA1 AH0 R R D Y\n",
      "\t IY1 L D AA1 N Z\n",
      "\t AO1 B R S T F F IH1 K AH0 K ER0 ER1 G B Y AE1 N T ER0 ER0\n",
      "\t AH1 N\n",
      "Epoch 61: train loss: 0.6877\tdev loss: 1.0606\n",
      "\tGenerated: in train: 6%, assess: 1%, novel: 93%\n",
      "\t AA1 T N\n",
      "\t EH1 V Z IH0 F Z\n",
      "\t EH1 L AA0 R AY1 V L AW1 AH0 K D\n",
      "\t EY1 HH OY2 N\n",
      "\t R EY2 L\n",
      "Epoch 62: train loss: 0.6866\tdev loss: 1.0591\n",
      "\tGenerated: in train: 14%, assess: 1%, novel: 85%\n",
      "\t AE1 P AA0 R ER0\n",
      "\t B\n",
      "\t K AE1 NG\n",
      "\t L AH0 S IY2\n",
      "\t ER1 HH K IY0 AE0 N\n",
      "Epoch 63: train loss: 0.6858\tdev loss: 1.0579\n",
      "\tGenerated: in train: 9%, assess: 3%, novel: 88%\n",
      "\t EH1 K D AA1 IY0\n",
      "\t IH1 L IH0 R R AO1 D D IH0 M IY0\n",
      "\t K M UH2 M IY0 IY0 IH0 M S N AH0 D IY0\n",
      "\t N M R AH0 AH0 IH0 L D AA1 D ER0\n",
      "\t L OW1 L\n",
      "Epoch 64: train loss: 0.6847\tdev loss: 1.0566\n",
      "\tGenerated: in train: 9%, assess: 2%, novel: 89%\n",
      "\t D AA1 P EH2 N P\n",
      "\t P EY1 AH0 K\n",
      "\t R OW1 M AH0 K K K T\n",
      "\t R IH1 L AH0 T\n",
      "\t AA1 S\n",
      "Epoch 65: train loss: 0.6837\tdev loss: 1.0552\n",
      "\tGenerated: in train: 8%, assess: 5%, novel: 87%\n",
      "\t NG M N\n",
      "\t B B R B EY1 IH1 S T IH0 T S S\n",
      "\t L AE1 N IY1 IY1 HH AO1 L Z EY2 K T EY0 Z\n",
      "\t R AH1 D AH0 N\n",
      "\t M IY0 T\n",
      "Epoch 66: train loss: 0.6829\tdev loss: 1.0540\n",
      "\tGenerated: in train: 5%, assess: 1%, novel: 94%\n",
      "\t B F EH1 R EH2 N ER2 L\n",
      "\t Z AH1 T S AH0 L S IY0\n",
      "\t S IH0 N\n",
      "\t N D\n",
      "\t K JH\n",
      "Epoch 67: train loss: 0.6822\tdev loss: 1.0531\n",
      "\tGenerated: in train: 4%, assess: 2%, novel: 94%\n",
      "\t R IH1 N IY0\n",
      "\t N AE1 L F T\n",
      "\t IH1 L IH0\n",
      "\t K IH1 N AA0 S D\n",
      "\t OW1 G\n",
      "Epoch 68: train loss: 0.6818\tdev loss: 1.0526\n",
      "\tGenerated: in train: 10%, assess: 0%, novel: 90%\n",
      "\t R EH1 S AH0 N T\n",
      "\t SH AA1 S OW0 T\n",
      "\t L IH1 N AH0 K\n",
      "\t P AH1 L ER1\n",
      "\t R S IH0 NG AY2\n",
      "Epoch 69: train loss: 0.6812\tdev loss: 1.0515\n",
      "\tGenerated: in train: 12%, assess: 0%, novel: 88%\n",
      "\t S T Y V OY2 AH0 S Z\n",
      "\t EY1 M D Y ER2 G T AH0 D L\n",
      "\t P AY1 N IH0 D IH0 D ER0 IY0 AH0 T L N Z\n",
      "\t S P K\n",
      "\t UW1 N B P IH1 OW1 N\n",
      "Epoch 70: train loss: 0.6804\tdev loss: 1.0506\n",
      "\tGenerated: in train: 6%, assess: 1%, novel: 93%\n",
      "\t R AA1 K ER0 B S V HH AH0 SH EH1 N ER0 IY0 D\n",
      "\t T D UW1 T T AH0 ER0 AY2 IH0 S\n",
      "\t R B AY1 S D AH0 D ER0 AH0 P\n",
      "\t SH EY1 K T AH0 K UW2 K T AH2 R AH0 L D M IY1 S\n",
      "\t AO1 M ER0 AH0 SH IH0 T UW0\n",
      "Epoch 71: train loss: 0.6799\tdev loss: 1.0499\n",
      "\tGenerated: in train: 8%, assess: 0%, novel: 92%\n",
      "\t R IH1 P\n",
      "\t F P IH0 W R ER1 K Z\n",
      "\t Y OW0 D AH0 S\n",
      "\t L Y AA1 HH AA1 L P EH1 K D\n",
      "\t T EH1 K NG IH0 S\n",
      "Epoch 72: train loss: 0.6795\tdev loss: 1.0492\n",
      "\tGenerated: in train: 8%, assess: 1%, novel: 91%\n",
      "\t AH1 N SH AH0 N\n",
      "\t N L\n",
      "\t M OW2 N M Z\n",
      "\t SH R ER1 P M IY0 T\n",
      "\t R R OW1 G\n",
      "Epoch 73: train loss: 0.6785\tdev loss: 1.0481\n",
      "\tGenerated: in train: 9%, assess: 3%, novel: 88%\n",
      "\t M AE1 S\n",
      "\t S S\n",
      "\t K M R S EH1 S\n",
      "\t Y AY0 S L\n",
      "\t IH1 K\n",
      "Epoch 74: train loss: 0.6776\tdev loss: 1.0467\n",
      "\tGenerated: in train: 11%, assess: 0%, novel: 89%\n",
      "\t N SH K AH0 L EH1 L\n",
      "\t Z M ER0 Z AH0 T AH0 N Z\n",
      "\t L UW0 OW0 IH0 N OW0 IY0 JH\n",
      "\t T G AE1 SH\n",
      "\t AY1 ER0 M AH0 W Z\n",
      "Epoch 75: train loss: 0.6772\tdev loss: 1.0461\n",
      "\tGenerated: in train: 10%, assess: 0%, novel: 90%\n",
      "\t L W OW1 R JH\n",
      "\t AH0 M AH0 B TH OY1 T AH0 S IY0\n",
      "\t EH1 N T\n",
      "\t K T W\n",
      "\t OW1 K\n",
      "Epoch 76: train loss: 0.6739\tdev loss: 1.0413\n",
      "\tGenerated: in train: 7%, assess: 0%, novel: 93%\n",
      "\t EH1 K EY2 ER0 T\n",
      "\t L EY1 D K IY0\n",
      "\t F AA1 S N\n",
      "\t B AA2 P B EH1 K EH1 HH K\n",
      "\t AA1 R B CH\n",
      "Epoch 77: train loss: 0.6717\tdev loss: 1.0381\n",
      "\tGenerated: in train: 9%, assess: 1%, novel: 90%\n",
      "\t W HH R AO2 R AE2 TH ER0 AH0 N M DH Z AH0\n",
      "\t L EY2 AH0 L N ER0\n",
      "\t L AE1 D IH0 L T AY2 N\n",
      "\t S T AH0 R S Z\n",
      "\t AO0 AA1 B L IH0 N AY0\n",
      "Epoch 78: train loss: 0.6658\tdev loss: 1.0291\n",
      "\tGenerated: in train: 12%, assess: 1%, novel: 87%\n",
      "\t S AO1 N AH0 EH1 CH S AH0\n",
      "\t N N R Z\n",
      "\t OW1 NG TH\n",
      "\t AA1 K ER2 CH L ER0 L\n",
      "\t NG\n",
      "Epoch 79: train loss: 0.6647\tdev loss: 1.0276\n",
      "\tGenerated: in train: 6%, assess: 0%, novel: 94%\n",
      "\t R UW2 N M\n",
      "\t B OW1 M AH0 M V\n",
      "\t N S AH0 Y AH1 K\n",
      "\t M JH\n",
      "\t Z R AH0 L\n",
      "Epoch 80: train loss: 0.6641\tdev loss: 1.0266\n",
      "\tGenerated: in train: 6%, assess: 4%, novel: 90%\n",
      "\t B AH1 CH W AE1 S\n",
      "\t T OY2 T S AO1 R IH0 IH0 D V AH0 V OY1\n",
      "\t SH S IY1 K IH0 S AH0 T\n",
      "\t AH0 N IY0\n",
      "\t SH IH1 K OY1 F IH0 K IY0\n",
      "Epoch 81: train loss: 0.6636\tdev loss: 1.0259\n",
      "\tGenerated: in train: 6%, assess: 3%, novel: 91%\n",
      "\t S B\n",
      "\t AH1 N S\n",
      "\t L AO1 F\n",
      "\t IH0 G S AH0 EH1 EY2 AH0 D\n",
      "\t N Z OW0\n",
      "Epoch 82: train loss: 0.6629\tdev loss: 1.0249\n",
      "\tGenerated: in train: 14%, assess: 1%, novel: 85%\n",
      "\t IY1 S\n",
      "\t T AO2 T SH AH0 L EH1 L T\n",
      "\t D R IH1 M IH0 M IY2 AY1 T\n",
      "\t S AH0 N AH0 D AH0 D ER0\n",
      "\t P AA1 T D ER0\n",
      "Epoch 83: train loss: 0.6625\tdev loss: 1.0243\n",
      "\tGenerated: in train: 12%, assess: 1%, novel: 87%\n",
      "\t IH1 L\n",
      "\t F IH1 L G IY0\n",
      "\t W AA1 Y AH0 ER0\n",
      "\t B EY1 B T Z\n",
      "\t AA0 D Z\n",
      "Epoch 84: train loss: 0.6617\tdev loss: 1.0234\n",
      "\tGenerated: in train: 9%, assess: 1%, novel: 90%\n",
      "\t N S Z L\n",
      "\t T AA1 B R\n",
      "\t K IH1 R K\n",
      "\t OW0 L\n",
      "\t R AE0 N R ER0 OW0\n",
      "Epoch 85: train loss: 0.6614\tdev loss: 1.0229\n",
      "\tGenerated: in train: 14%, assess: 1%, novel: 85%\n",
      "\t B\n",
      "\t R IH1 L\n",
      "\t S R\n",
      "\t P K IY1 K ER0 AH0 IY2 M\n",
      "\t S K IY1 AH0 IH1 G IY0\n",
      "Epoch 86: train loss: 0.6610\tdev loss: 1.0222\n",
      "\tGenerated: in train: 19%, assess: 1%, novel: 80%\n",
      "\t M EH1 S\n",
      "\t T EH1 T R EH0 S N CH\n",
      "\t F W\n",
      "\t N AH0 T\n",
      "\t T OW1 V\n",
      "Epoch 87: train loss: 0.6605\tdev loss: 1.0215\n",
      "\tGenerated: in train: 13%, assess: 1%, novel: 86%\n",
      "\t AH0 N AH0 V\n",
      "\t AA1 N\n",
      "\t B EH1 F S AH0\n",
      "\t M EH1 Z\n",
      "\t AH0 W Y CH AE1 AH1 NG AH0 Z\n",
      "Epoch 88: train loss: 0.6600\tdev loss: 1.0209\n",
      "\tGenerated: in train: 13%, assess: 1%, novel: 86%\n",
      "\t M IH1 T AH0 S T N\n",
      "\t R EH2 D T IH1 N\n",
      "\t N L IH0 SH AH0 IY0 AE1 K T\n",
      "\t D IH0 T ER1 N D\n",
      "\t S AY1 G AH0 V AH0 N T\n",
      "Epoch 89: train loss: 0.6593\tdev loss: 1.0200\n",
      "\tGenerated: in train: 14%, assess: 2%, novel: 84%\n",
      "\t L AH1 L AW2 B\n",
      "\t R OW0 M ER1 D\n",
      "\t HH ER0 AH1 N\n",
      "\t V AE1 L\n",
      "\t AA1 AH0 K N\n",
      "Epoch 90: train loss: 0.6590\tdev loss: 1.0195\n",
      "\tGenerated: in train: 11%, assess: 1%, novel: 88%\n",
      "\t B AA1 K Z EY2\n",
      "\t R UW1 S JH\n",
      "\t AH0 M ER0 P\n",
      "\t AY1 P D\n",
      "\t S S ER1 K S OW0\n",
      "Epoch 91: train loss: 0.6586\tdev loss: 1.0190\n",
      "\tGenerated: in train: 8%, assess: 1%, novel: 91%\n",
      "\t F OW1 N EY0 OY2\n",
      "\t F IH1 IY0\n",
      "\t D AE1 NG B IH0 B AH0 M IY0\n",
      "\t N AH1 B B L\n",
      "\t S IY1 M L AO2 S\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92: train loss: 0.6584\tdev loss: 1.0188\n",
      "\tGenerated: in train: 13%, assess: 3%, novel: 84%\n",
      "\t ER0 Y AH0 L AH0 K\n",
      "\t M AY1 NG P\n",
      "\t JH AA2 F AH0 D AH0 N\n",
      "\t SH IH1 N L AE0 L IH0 K M IY1 S AH0 D\n",
      "\t N\n",
      "Epoch 93: train loss: 0.6579\tdev loss: 1.0180\n",
      "\tGenerated: in train: 9%, assess: 1%, novel: 90%\n",
      "\t G AO1 Z\n",
      "\t K DH UW1 HH ER0 IY0 OW0 OW0 IH0 K IH1 JH L\n",
      "\t G R AA1 N\n",
      "\t CH EH1 NG NG AH0 K ER0 T EH2 N L\n",
      "\t EY1 N AH0 EH1 EH0 L AY2 IY0 AH0 Z IH0 K TH IY0\n",
      "Epoch 94: train loss: 0.6575\tdev loss: 1.0175\n",
      "\tGenerated: in train: 9%, assess: 0%, novel: 91%\n",
      "\t EH1 S AH0 P R Z D\n",
      "\t R M UH1 L ER0 W\n",
      "\t AH0 D D Z AH0 M G IH0 G\n",
      "\t B OW1 R W ER0 M EH1 NG ER0\n",
      "\t AH1 F IY0\n",
      "Epoch 95: train loss: 0.6571\tdev loss: 1.0169\n",
      "\tGenerated: in train: 12%, assess: 1%, novel: 87%\n",
      "\t L AA1 R\n",
      "\t T IY1 B OW1 N\n",
      "\t SH\n",
      "\t S AH0 V Z L AH0 CH\n",
      "\t S AE1 L V HH IY1 AA2\n",
      "Epoch 96: train loss: 0.6567\tdev loss: 1.0164\n",
      "\tGenerated: in train: 8%, assess: 0%, novel: 92%\n",
      "\t K IH0 R IH0 N AH0 L ER0\n",
      "\t UW1 R AH0 K OW0 M\n",
      "\t HH EH1 V AH0 G IH0 IH2 M AH0 G UH1 B N\n",
      "\t M Y F AO1 TH\n",
      "\t T AA1 K T\n",
      "Epoch 97: train loss: 0.6564\tdev loss: 1.0160\n",
      "\tGenerated: in train: 15%, assess: 1%, novel: 84%\n",
      "\t EH1 R IY0\n",
      "\t V T T B UH1 N D\n",
      "\t L AY1 L AA2 N Z S\n",
      "\t DH IY1 S Z\n",
      "\t N\n",
      "Epoch 98: train loss: 0.6556\tdev loss: 1.0148\n",
      "\tGenerated: in train: 8%, assess: 2%, novel: 90%\n",
      "\t R IH1 R G M AH0 P IY0 ER0 IY0\n",
      "\t IH0 D AH0 N\n",
      "\t IY0\n",
      "\t K AA1 JH D\n",
      "\t T AH0 G AE1 G IH0 F N\n",
      "Epoch 99: train loss: 0.6552\tdev loss: 1.0143\n",
      "\tGenerated: in train: 7%, assess: 3%, novel: 90%\n",
      "\t N N\n",
      "\t R IY0 M AH0 V AH0 SH CH F IH1 D OW0 IY0\n",
      "\t P D AO1 N\n",
      "\t M AE1 N IH0 G IY0 Z\n",
      "\t R AH1 F AH0 D\n",
      "Epoch 100: train loss: 0.6549\tdev loss: 1.0138\n",
      "\tGenerated: in train: 11%, assess: 0%, novel: 89%\n",
      "\t SH Y EY1 D AH0 N\n",
      "\t B AE1 S D EH2 N UW1 T\n",
      "\t AH0 K IY0\n",
      "\t NG T F D AA0 S AH0 R\n",
      "\t SH AY1 TH\n",
      "Epoch 101: train loss: 0.6547\tdev loss: 1.0135\n",
      "\tGenerated: in train: 11%, assess: 0%, novel: 89%\n",
      "\t S AA0 P K AO2 P N T AH0 T IH0 T AE1 N\n",
      "\t M HH AA1 D AH0 S AH0 D AO2 G B\n",
      "\t AA0 D AO1 W AH0 N L\n",
      "\t M IY1 L AH0 JH OW0 T AY2 N\n",
      "\t TH K\n",
      "Epoch 102: train loss: 0.6544\tdev loss: 1.0131\n",
      "\tGenerated: in train: 9%, assess: 0%, novel: 91%\n",
      "\t IY1 S\n",
      "\t L AH1 G\n",
      "\t B UH1 G AH0 K\n",
      "\t L ER1 L L\n",
      "\t HH AY1 R AH0 T V EH1 S M\n",
      "Epoch 103: train loss: 0.6542\tdev loss: 1.0127\n",
      "\tGenerated: in train: 12%, assess: 0%, novel: 88%\n",
      "\t S IH0 B R AA1 NG ER0 T\n",
      "\t F JH IH1 M L\n",
      "\t R IY1 L R\n",
      "\t K ER0 G EH1 S P T S D AA2 R IY0\n",
      "\t T ER0 L AA1 N AH0 AH1 R T ER0 AH0 Z ER0\n",
      "Epoch 104: train loss: 0.6540\tdev loss: 1.0125\n",
      "\tGenerated: in train: 17%, assess: 0%, novel: 83%\n",
      "\t F IY1 K IH0 N R\n",
      "\t K IY1 S IH1 S M W AO1 T D IY2 S Z\n",
      "\t M AA1 R\n",
      "\t EH1 S AH0 N\n",
      "\t EY2 N N AH0 T\n",
      "Epoch 105: train loss: 0.6538\tdev loss: 1.0121\n",
      "\tGenerated: in train: 8%, assess: 3%, novel: 89%\n",
      "\t T OW0 IH1 S IY1 S\n",
      "\t L HH AH1 S AA0 N\n",
      "\t R P AO1 M ER0 IH0 S R AH0 G\n",
      "\t W\n",
      "\t IY1 K R\n",
      "Epoch 106: train loss: 0.6535\tdev loss: 1.0117\n",
      "\tGenerated: in train: 12%, assess: 1%, novel: 87%\n",
      "\t T R ER1 JH IY0\n",
      "\t F EY1 F M IY0 ER0\n",
      "\t R AO2 M IH0 N ER0\n",
      "\t P EY1 N\n",
      "\t T R IH1 N D AH0\n",
      "Epoch 107: train loss: 0.6532\tdev loss: 1.0113\n",
      "\tGenerated: in train: 13%, assess: 3%, novel: 84%\n",
      "\t M W IH1 AH0 B Z\n",
      "\t L UW1 L IH0 K AH0 NG AY2 TH\n",
      "\t L R EH1 N N\n",
      "\t P T IH1 S D\n",
      "\t IY0 R K EY2 AH0 AH0 K IH1 L\n",
      "Epoch 108: train loss: 0.6530\tdev loss: 1.0109\n",
      "\tGenerated: in train: 12%, assess: 0%, novel: 88%\n",
      "\t M\n",
      "\t T IY1 D IH0 NG CH IY2 JH\n",
      "\t R AA1 N G ER0 T\n",
      "\t IH0 S Z IY0\n",
      "\t P AY1 N\n",
      "Epoch 109: train loss: 0.6528\tdev loss: 1.0107\n",
      "\tGenerated: in train: 6%, assess: 0%, novel: 94%\n",
      "\t W AA1 N K D AH0 M S W ER0 EY1 N T IH0 N S\n",
      "\t D UW1 N EY2 IY0 Z AH0 S IH1 R IH0 L R AY2 D\n",
      "\t F AH0 D IH1 NG\n",
      "\t S B\n",
      "\t R AY1 L AH0 R\n",
      "Epoch 110: train loss: 0.6526\tdev loss: 1.0104\n",
      "\tGenerated: in train: 15%, assess: 1%, novel: 84%\n",
      "\t IH1 N\n",
      "\t M ER1 N\n",
      "\t T IY1 R V AY1 AA1 P IH1 TH S OW1 L\n",
      "\t R ER0 AH1 K T IY0\n",
      "\t S EH1 K S\n",
      "Epoch 111: train loss: 0.6524\tdev loss: 1.0101\n",
      "\tGenerated: in train: 13%, assess: 1%, novel: 86%\n",
      "\t L AE1 R R\n",
      "\t R L M AE1 L IY0 D ER0\n",
      "\t K EH1 N Z\n",
      "\t P AH1 M T AH0 R S\n",
      "\t SH\n",
      "Epoch 112: train loss: 0.6523\tdev loss: 1.0100\n",
      "\tGenerated: in train: 19%, assess: 0%, novel: 81%\n",
      "\t EY1 N S N ER0 P\n",
      "\t P EH1 N\n",
      "\t M K AA1 V ER0 Z\n",
      "\t N S R IH0 N IH0 K UW AH0 L IY0\n",
      "\t R IY1 G\n",
      "Epoch 113: train loss: 0.6517\tdev loss: 1.0092\n",
      "\tGenerated: in train: 11%, assess: 1%, novel: 88%\n",
      "\t S R AE1 S T F T ER0 T IH0 L AY2\n",
      "\t T AE1 T\n",
      "\t R AA1 L EH1\n",
      "\t EY1 G S IY0\n",
      "\t HH AH1 L OW0 T\n",
      "Epoch 114: train loss: 0.6516\tdev loss: 1.0090\n",
      "\tGenerated: in train: 11%, assess: 1%, novel: 88%\n",
      "\t T IH1 M AE1 IH0 M\n",
      "\t R IY1 S R AH0 CH M IY0\n",
      "\t B IH1 AE1 G\n",
      "\t EY1 N AH0 K IH0 F\n",
      "\t OW1 N\n",
      "Epoch 115: train loss: 0.6513\tdev loss: 1.0087\n",
      "\tGenerated: in train: 10%, assess: 0%, novel: 90%\n",
      "\t OY0 R K EY2\n",
      "\t R AA1 F IH0 S T P\n",
      "\t R AE1 T\n",
      "\t OW0 Z EY2 S AH0 G AH0 Z IH0 N IY0\n",
      "\t R AE1 T T ER0 IY0 ER0 T AH0\n",
      "Epoch 116: train loss: 0.6512\tdev loss: 1.0085\n",
      "\tGenerated: in train: 14%, assess: 1%, novel: 85%\n",
      "\t G IY1 F CH\n",
      "\t UH1 T W IY0 Z S\n",
      "\t Y AH1 M OW0 D L AH0 JH\n",
      "\t P EH2 R AO1 N L\n",
      "\t P OW0 T R AE1 K\n",
      "Epoch 117: train loss: 0.6510\tdev loss: 1.0081\n",
      "\tGenerated: in train: 3%, assess: 1%, novel: 96%\n",
      "\t L AE1 N AA2\n",
      "\t S R EH1 EY1 NG\n",
      "\t Y AH1 S\n",
      "\t R AA2 HH EH0 NG\n",
      "\t L S W ER0 P AA1 R\n",
      "Epoch 118: train loss: 0.6508\tdev loss: 1.0079\n",
      "\tGenerated: in train: 11%, assess: 0%, novel: 89%\n",
      "\t OW2 D G AH0 T AH0 D AE2 L\n",
      "\t IH1 B EH1 G\n",
      "\t V AE1 S D ER0 AH0 F D IH0 L T\n",
      "\t AA1 R Z\n",
      "\t AE1 S ER0\n",
      "Epoch 119: train loss: 0.6507\tdev loss: 1.0077\n",
      "\tGenerated: in train: 5%, assess: 3%, novel: 92%\n",
      "\t N T\n",
      "\t R AH0 EH1 N\n",
      "\t OW1 N\n",
      "\t L IH0 W IH1 Z IY0 OW0\n",
      "\t HH M EH2 HH ER1 S OW1 T Y AH0\n",
      "Epoch 120: train loss: 0.6505\tdev loss: 1.0076\n",
      "\tGenerated: in train: 12%, assess: 0%, novel: 88%\n",
      "\t R AE1 R\n",
      "\t K IY1 T IH2 B IH1 V R Y\n",
      "\t IH1 TH T AO0 T IH0 T OW2 S IY2 V AH0 N IY2 T W AY1 R\n",
      "\t K IH1 R ZH IH1\n",
      "\t AE1 L AH0 G T\n",
      "Epoch 121: train loss: 0.6502\tdev loss: 1.0071\n",
      "\tGenerated: in train: 13%, assess: 4%, novel: 83%\n",
      "\t R IY1 L\n",
      "\t R\n",
      "\t R IH1 S S\n",
      "\t N N\n",
      "\t D ER0 P IY0 K K S AH0 K Z\n",
      "Epoch 122: train loss: 0.6501\tdev loss: 1.0069\n",
      "\tGenerated: in train: 18%, assess: 4%, novel: 78%\n",
      "\t N V IH0 S M\n",
      "\t L R AE1 L IY0\n",
      "\t L Y UW1 P P R AH1 N S AH1 N ER0 S AH0 N\n",
      "\t JH AH1 T AO1 F R AH0\n",
      "\t R IY0 V\n",
      "Epoch 123: train loss: 0.6500\tdev loss: 1.0067\n",
      "\tGenerated: in train: 7%, assess: 3%, novel: 90%\n",
      "\t L AH0 UW1 L\n",
      "\t B B EH1 S\n",
      "\t B AE1 M R\n",
      "\t IH1 N N T EH0\n",
      "\t IH1 Z\n",
      "Epoch 124: train loss: 0.6498\tdev loss: 1.0065\n",
      "\tGenerated: in train: 22%, assess: 0%, novel: 78%\n",
      "\t N T AH0 R IH0 SH AH0 N\n",
      "\t L AH1 N\n",
      "\t P Y AY1 L EH1 NG AE1 AH0 AH0 N\n",
      "\t P AH1 L AH0 N Z\n",
      "\t AA1 K\n",
      "Epoch 125: train loss: 0.6496\tdev loss: 1.0062\n",
      "\tGenerated: in train: 10%, assess: 1%, novel: 89%\n",
      "\t IH2 N L\n",
      "\t G AH0 M AE1 M S AO0 K\n",
      "\t SH AA1 AH0 G IH0 N Z IY0\n",
      "\t T EH1 NG IH0 OW1 L\n",
      "\t V AY1 L AH0 AH0 M P\n",
      "Epoch 126: train loss: 0.6495\tdev loss: 1.0059\n",
      "\tGenerated: in train: 11%, assess: 0%, novel: 89%\n",
      "\t M ER1 P ER0 L\n",
      "\t R EH1 N T AH0 AW2 L ER0\n",
      "\t R Y EH1 K M AH0 Z AH0 T AH0 S\n",
      "\t K EH1 K AE2 L M Z\n",
      "\t K EY1 N T ER0\n",
      "Epoch 127: train loss: 0.6495\tdev loss: 1.0060\n",
      "\tGenerated: in train: 14%, assess: 3%, novel: 83%\n",
      "\t R ER2 S T AY1\n",
      "\t K IH1 T\n",
      "\t G OW1 JH AO0 R Z Z IY0\n",
      "\t R AA1 T ER0\n",
      "\t L IH1 K AO1 N\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Early stopping because of no decrease in 1 epochs.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'num_parameters' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'num_parameters' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "param_grid = ParameterGrid({\n",
    "    'embedding_dimension': [50],\n",
    "    'hidden_dimension': [2],\n",
    "})\n",
    "\n",
    "records = []\n",
    "for params in tqdm(param_grid):\n",
    "    model_parameters = ModelParams(rnn_type='rnn', num_layers=1, max_epochs=1000, early_stopping_rounds=1, **params)\n",
    "    model = LanguageModel(vocab, model_parameters, device_name='cpu')\n",
    "\n",
    "    print('Model Params:', model_parameters)\n",
    "    \n",
    "    train_losses, dev_losses = model.fit(\n",
    "        train_pronunciations.pronunciation.values.tolist(),\n",
    "        dev_pronunciations.pronunciation.values.tolist()\n",
    "    )\n",
    "    \n",
    "    for epoch, (train_loss, dev_loss) in enumerate(zip(train_losses, dev_losses), start=1):\n",
    "        record = params.copy()\n",
    "        record['epoch'] = epoch\n",
    "        record['train_loss'] = train_loss\n",
    "        record['dev_loss'] = dev_loss\n",
    "    \n",
    "        records.append(record)\n",
    "\n",
    "models_df = pd.DataFrame.from_records(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_df = pd.DataFrame.from_records(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'embedding_dimension'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-6c60a1d2eaae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodels_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'embedding_dimension'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'hidden_dimension'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'num_layers'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdev_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/code/sonorous/venv/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mgroupby\u001b[0;34m(self, by, axis, level, as_index, sort, group_keys, squeeze, observed, **kwargs)\u001b[0m\n\u001b[1;32m   7892\u001b[0m             \u001b[0msqueeze\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7893\u001b[0m             \u001b[0mobserved\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobserved\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7894\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7895\u001b[0m         )\n\u001b[1;32m   7896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/sonorous/venv/lib/python3.7/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36mgroupby\u001b[0;34m(obj, by, **kwds)\u001b[0m\n\u001b[1;32m   2520\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"invalid type: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2522\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mklass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/code/sonorous/venv/lib/python3.7/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, squeeze, observed, **kwargs)\u001b[0m\n\u001b[1;32m    389\u001b[0m                 \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m                 \u001b[0mobserved\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobserved\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m                 \u001b[0mmutated\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmutated\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m             )\n\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/sonorous/venv/lib/python3.7/site-packages/pandas/core/groupby/grouper.py\u001b[0m in \u001b[0;36m_get_grouper\u001b[0;34m(obj, key, axis, level, sort, observed, mutated, validate)\u001b[0m\n\u001b[1;32m    619\u001b[0m                 \u001b[0min_axis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 621\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    622\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGrouper\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mgpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m             \u001b[0;31m# Add key to exclusions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'embedding_dimension'"
     ]
    }
   ],
   "source": [
    "models_df.groupby(['embedding_dimension', 'hidden_dimension', 'num_layers']).dev_loss.min().sort_values().to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are {meow} models with a dev error of around .76. I'll choose the simplest one, which \n",
    "\n",
    "* point out that no matter how low the train error gets, the dev error\n",
    "* which model parameters fail to ever get to the lowest dev error\n",
    "* which model parameters overfit the most\n",
    "\n",
    "* isolate the group of models with about .76 dev error. choose the simplest one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = models_df[(models_df.embedding_dimension==50) & (models_df.hidden_dimension==100) & (models_df.num_layers==3)]\n",
    "t = t.set_index('epoch')\n",
    "t.dev_loss.plot()\n",
    "t.train_loss.plot()\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_df.sort_values('dev_loss').iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the best \"model\" was at an earlier epoch I don't have access to it. So I'll train a model with that model's parameters and set the number of epochs to MEOW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "lm = PhonemeLM(\n",
    "    phoneme_to_idx, device='cuda', rnn_type='gru',\n",
    "    embedding_dimension=50, hidden_dimension=100, num_layers=3,\n",
    "    max_epochs=69, early_stopping_rounds=69, batch_size=1024,\n",
    ")\n",
    "\n",
    "train_loss, dev_loss = lm.fit(train_df.pronunciation.values.tolist(), dev_df.pronunciation.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = build_data_loader(test_df.pronunciation.values.tolist(), lm.phoneme_to_idx)\n",
    "lm.evaluate(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_loader = build_data_loader(dev_df.pronunciation.values.tolist(), lm.phoneme_to_idx)\n",
    "lm.evaluate(dev_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* compute the test error for the final model. plot the train, dev, and test errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Final Model ##\n",
    "Now that we've found the best parameters for the model according to the dev set we'll train a final model using all of the data. This should increase model performance overall since more data is better, but is also necessary since we'll be using the model to predict probabilities of all English words below. If some of those words weren't in the training set they would artificially get lower probabilities. (Another approach here could be to train a model on e.g. 4/5 folds of the data and make predictions about the remaining 1/5, doing that 5 times to get unbiased predictions for all data, but this would have taken much longer to run.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model_params = ModelParams(\n",
    "    rnn_type='gru', embedding_dimension=50, hidden_dimension=100, num_layers=3,\n",
    "    max_epochs=69, early_stopping_rounds=69, batch_size=1024\n",
    ")\n",
    "\n",
    "language_model = LanguageModel(vocab, model_params, device_name='cpu')\n",
    "\n",
    "train_loss, dev_loss = language_model.fit(df.pronunciation.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lm_50_2.pt', 'wb') as fh:\n",
    "    model.save(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
