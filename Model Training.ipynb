{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Phoneme Language Model #\n",
    "In this notebook I train a language model over English sounds (also known as [phonemes](https://en.wikipedia.org/wiki/Phoneme)). The data for English pronunciations comes from the CMU Pronouncing Dictionary. The pronunciations in the pronouncing dictionary are in [ARPABET](https://en.wikipedia.org/wiki/ARPABET), a set of symbols representing English sounds. So in ARPABET \"fish\" is pronounced as /F IH1 SH/.\n",
    "\n",
    "By training on tens of thousands of pronunciations the model will hopefully learn [English phonotactics](https://en.wikipedia.org/wiki/Phonotactics#English_phonotactics), the rules that govern what sounds like a valid English word. For example, /F AH1 N/ (\"fun\") sounds good, but /NG S ER1/ (maybe represented as \"ngsr\") does not.\n",
    "\n",
    "\n",
    "Check out the notebook `Phoneme Exploration.ipynb` if you want to see the model used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from torch.nn import functional as F\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sonorous.languagemodel import LanguageModel, ModelParams, Vocabulary\n",
    "from sonorous.pronunciationdata import load_pronunciations\n",
    "from sonorous.utils import split_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data ##\n",
    "The data for this model comes from the [CMU Pronouncing Dictionary](http://www.speech.cs.cmu.edu/cgi-bin/cmudict), which contains over one hundred thousand pronunciations. Each pronuncation is in [ARPABET](https://en.wikipedia.org/wiki/ARPABET), a set of symbols for representing English speech sounds. In ARPABET the word \"fish\" is represented by the sequence of phonemes /F IH1 SH/. You can probably guess the first and third sounds. The vowel in the middle has \"1\" at the end to indicate it has the primary stress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll use the `load_pronunciations` function to load the Pronouncing Dictionary into a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 131,964 pronunciations.\n",
      "\n",
      "Sample of 5 pronunciations:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pronunciation</th>\n",
       "      <th>as_string</th>\n",
       "      <th>num_phonemes</th>\n",
       "      <th>num_syllables</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>layoff</th>\n",
       "      <td>(ˈ, l, eɪ, ˌ, ɔ, f)</td>\n",
       "      <td>ˈleɪˌɔf</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>congolese</th>\n",
       "      <td>(ˌ, k, ɑː, n, g, ə, ˈ, l, iː, z)</td>\n",
       "      <td>ˌkɑːngəˈliːz</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prefers</th>\n",
       "      <td>(p, r, ɪ, ˈ, f, ɝː, z)</td>\n",
       "      <td>prɪˈfɝːz</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nader</th>\n",
       "      <td>(ˈ, n, eɪ, d, ɝ)</td>\n",
       "      <td>ˈneɪdɝ</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rollins's</th>\n",
       "      <td>(ˈ, r, ɑː, l, ɪ, n, z, ɪ, z)</td>\n",
       "      <td>ˈrɑːlɪnzɪz</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              pronunciation     as_string  num_phonemes  \\\n",
       "word                                                                      \n",
       "layoff                  (ˈ, l, eɪ, ˌ, ɔ, f)       ˈleɪˌɔf             4   \n",
       "congolese  (ˌ, k, ɑː, n, g, ə, ˈ, l, iː, z)  ˌkɑːngəˈliːz             8   \n",
       "prefers              (p, r, ɪ, ˈ, f, ɝː, z)      prɪˈfɝːz             6   \n",
       "nader                      (ˈ, n, eɪ, d, ɝ)        ˈneɪdɝ             4   \n",
       "rollins's      (ˈ, r, ɑː, l, ɪ, n, z, ɪ, z)    ˈrɑːlɪnzɪz             8   \n",
       "\n",
       "           num_syllables  \n",
       "word                      \n",
       "layoff                 2  \n",
       "congolese              3  \n",
       "prefers                2  \n",
       "nader                  2  \n",
       "rollins's              3  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pronunciations = load_pronunciations()\n",
    "print(f\"There are {len(pronunciations):,} pronunciations.\")\n",
    "print()\n",
    "print(\"Sample of 5 pronunciations:\")\n",
    "pronunciations.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the pronunciation for \"fish\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pronunciation    (ˈ, f, ɪ, ʃ)\n",
       "as_string                ˈfɪʃ\n",
       "num_phonemes                3\n",
       "num_syllables               1\n",
       "Name: fish, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pronunciations.loc['fish']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here are all of the pronunciations for the word \"tomato\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pronunciation</th>\n",
       "      <th>as_string</th>\n",
       "      <th>num_phonemes</th>\n",
       "      <th>num_syllables</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tomato</th>\n",
       "      <td>(t, ə, ˈ, m, eɪ, ˌ, t, oʊ)</td>\n",
       "      <td>təˈmeɪˌtoʊ</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tomato</th>\n",
       "      <td>(t, ə, ˈ, m, ɑː, ˌ, t, oʊ)</td>\n",
       "      <td>təˈmɑːˌtoʊ</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     pronunciation   as_string  num_phonemes  num_syllables\n",
       "word                                                                       \n",
       "tomato  (t, ə, ˈ, m, eɪ, ˌ, t, oʊ)  təˈmeɪˌtoʊ             6              3\n",
       "tomato  (t, ə, ˈ, m, ɑː, ˌ, t, oʊ)  təˈmɑːˌtoʊ             6              3"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pronunciations.loc['tomato']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model ##\n",
    "The module `languagemodel.py` contains a class `LanguageModel` that implements a simple neural language model. It's a PyTorch neural network comprised of the following layers:\n",
    "1. **Embedding layer** to translate each phoneme into a dense vector. Note that in the code this is called the _encoder since it encodes input phonemes into a representation the model can work with.\n",
    "2. An recurrent neural network (**RNN**) layer that processes each input phoneme sequentially and for each step generates (a) a hidden representation to pass on to the next step and (b) an output.\n",
    "3. A **linear layer** that decodes the outputes (2b) into distributions over each phoneme. Note that in the code this is called the _docoder since it decodes the model's internal representations back into phonemes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go through a simple example of what happens when we pass the pronunciation /F IH1 SH/ through the model. Ultimately what I want ouf of the model is a prediction at each position of what the next phoneme should be. For example, when a well trained model is sees /F IH1/ it should know that /SH/ is likely, or at least not unlikely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First I'll create a `Vocabulary` instance by passing in all the pronunciations. The `vocab` is used to convert phonemes into integer indices that the neural network handle. It does a few other things too, which you can see below. The `Vocabulary` class's code is in `sonorous/languagemodel.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ˈfɪʃ\n"
     ]
    }
   ],
   "source": [
    "print(''.join(pronunciations.loc['fish'].pronunciation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 47 distinct phonemes.\n",
      "\n",
      "Looking up the int index for /ʃ/: 31\n",
      "\n",
      "Checking whether /ʃ/ is in the vocabulary: True\n",
      "\n",
      "Looking up the phoneme for a specific int index: ʃ\n",
      "\n",
      "Encoding /ˈfɪʃ/: [ 1  5 13 22 31  2]\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocabulary.from_texts(pronunciations.pronunciation.values)\n",
    "\n",
    "print(f\"There are {len(vocab)} distinct phonemes.\")\n",
    "print()\n",
    "print(\"Looking up the int index for /ʃ/:\", vocab['ʃ'])\n",
    "print()\n",
    "print(\"Checking whether /ʃ/ is in the vocabulary:\", 'ʃ' in vocab)\n",
    "print()\n",
    "print(\"Looking up the phoneme for a specific int index:\", vocab.token_from_idx(vocab['ʃ']))\n",
    "print()\n",
    "print(\"Encoding /ˈfɪʃ/:\", vocab.encode_text(tuple(\"ˈfɪʃ\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'll define the model. Note that I'm not actually fitting the model to any data so the output will be random. The hyperparameters aren't optimal, but again that doesn't matter here since I just want to show the flow of data through the network.\n",
    "\n",
    "The `ModelParams` class (from `sonorous/languagemodel.py` encapsulates hyperparameters and options for the model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = ModelParams(\n",
    "    rnn_type='rnn', embedding_dimension=10, hidden_dimension=3, num_layers=1,\n",
    "    max_epochs=3, early_stopping_rounds=3\n",
    ")\n",
    "\n",
    "language_model = LanguageModel(vocab, model_params, 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'll walk through what happens when we pass the word \"fish\" /F IH1 SH/ through the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fish_pronunciation = tuple(\"ˈfɪʃ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Vocabulary.encode_text` function we saw earlier does a few things. First, it adds dummy `<START>` and `<END>` tokens to the pronunciation indicating its start and end. This allows the model to learn transition probabilities from the start of the word to the first phoneme, and from the last phoneme to the end of the word.\n",
    "\n",
    "It then converts every phoneme to its ingeter index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  5 13 22 31  2]\n",
      "\n",
      "1 => <START>\n",
      "5 => ˈ\n",
      "13 => f\n",
      "22 => ɪ\n",
      "31 => ʃ\n",
      "2 => <END>\n"
     ]
    }
   ],
   "source": [
    "fish_input = vocab.encode_text(fish_pronunciation)\n",
    "print(fish_input)\n",
    "print()\n",
    "for idx in fish_input:\n",
    "    phoneme = vocab.token_from_idx(idx)\n",
    "    print(f'{idx} => {phoneme}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we're ready to pass the input into the model's `forward` function, which takes in inputs and outputs predictions. This model's `forward` function expects a Tensor of dimension `(batch_size, NUMBER OF STEPS)`. A step here refers to a step forward in the sequence, so /<START> F IH1 SH <END>/ has 5 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input's shape: (6,)\n",
      "Batch input's shape: torch.Size([1, 6])\n"
     ]
    }
   ],
   "source": [
    "print(\"Input's shape:\", fish_input.shape)\n",
    "fish_batch_input = torch.LongTensor(fish_input).unsqueeze(0)\n",
    "print(\"Batch input's shape:\", fish_batch_input.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing the `forward` function does is embed each phoneme using an [nn.Embedding](https://pytorch.org/docs/stable/nn.html#embedding). Each phoneme has a dedicated embedding vector of length `embedding_dimension`, so the shape of `embedded` is `(batch size, number of steps, embedding_dimension)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 10])\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.8161, -0.8673, -0.8075,  2.0873, -0.3538,  0.7794, -1.1596,\n",
       "          -0.1017,  0.7367, -0.1752],\n",
       "         [-0.7299, -0.2113,  0.0539,  1.2842,  0.8718, -0.1122, -0.1860,\n",
       "           0.4326, -0.8699,  1.6186],\n",
       "         [-1.8333,  0.9768, -1.2780,  1.6892, -0.1444, -0.0279, -0.3749,\n",
       "           1.2191, -1.2809,  0.8605],\n",
       "         [-0.2274, -0.2111, -1.1193, -0.1698,  0.8556,  0.6363, -0.0765,\n",
       "           0.1790, -0.8408,  1.4178],\n",
       "         [-0.9943, -0.5585, -0.8412,  1.6460,  0.6196,  1.0931,  1.4780,\n",
       "          -1.3041,  0.4240, -1.7979],\n",
       "         [-1.7650, -0.5110, -1.6025,  1.0763, -0.5914, -1.4563,  0.3198,\n",
       "           1.6319, -0.7680, -2.1770]]], grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded = language_model._encoder(fish_batch_input)\n",
    "print(embedded.shape)\n",
    "print()\n",
    "embedded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll pass `embedded` to the [RNN layer](https://pytorch.org/docs/stable/nn.html#recurrent-layers), resulting in `rnn_output` and `hidden_state`. I won't go into detail on how RNNs work since there are many detailed posts on the web you can read, but the basic idea is a cell is applied sequentially to every token (i.e. step) in the input. At each step an output and a hidden state are produced. The hidden state can be passed on to the next step, and the output can be used to make a prediction.\n",
    "\n",
    "The `rnn` layer below operates on the full sequence, so the results are for the entire sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 3])\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2904,  0.9463, -0.4982],\n",
       "         [-0.7268, -0.8107, -0.6783],\n",
       "         [-0.9655,  0.5736, -0.6357],\n",
       "         [-0.7353, -0.4838, -0.0497],\n",
       "         [-0.9659,  0.9752,  0.2446],\n",
       "         [-0.9720,  0.8659, -0.0780]]], grad_fn=<TransposeBackward1>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_output, hidden_state = language_model._rnn(embedded)\n",
    "print(rnn_output.shape)\n",
    "print()\n",
    "rnn_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our application we can ignore the `hidden_state`-- the `rnn_output` is the interesting part. The first dimension is for the batch, and we only have a single input in our batch. The second dimension is for each of the input phonemes. The third dimension corresponds to `hidden_dimension`: you can think of this as the state of the RNN at each step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I said at the beginning of this section, I want the output of the RNN at each position to be predictions for the *next* position. So I'll apply a [linear layer](https://pytorch.org/docs/stable/nn.html#linear) to the `rnn_output`, resulting in a vector the size of the vocabularly at each position. The [softmax](https://pytorch.org/docs/stable/nn.functional.html#softmax) function normalizes the outputs into probability distributions for each prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 47])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = language_model._decoder(rnn_output)\n",
    "probabilities = F.softmax(outputs, dim=-1).squeeze()\n",
    "probabilities.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of `probabilities` is (5, 42) because each of the five tokens in /ˈfɪʃ/ gets a a probability distribution over each of the 42 phonemes in the vocabulary.\n",
    "\n",
    "The first phoneme in the input is the `<START>` token; let's see what the model thinks should come next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'k'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probabilities_for_first_phoneme = probabilities[0]\n",
    "most_likely_first_phoneme_idx = probabilities_for_first_phoneme.argmax().item()\n",
    "most_likely_first_phoneme = vocab.token_from_idx(most_likely_first_phoneme_idx)\n",
    "most_likely_first_phoneme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the model predicts /ŋ/ to be the first phoneme in the word. Since the model isn't fit yet this is just a random guess. In order to get the model to make good predictions I'll need to first train a good model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the Best Model ##\n",
    "In this section I'll train a number of models on the train set and select the one that has the lowest error on the dev set. I'll split the DataFrame of pronunciations into three DataFrames, with 79% for training, 20% for dev/validation, and 1% for testing of the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(104251, 26393, 1320)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pronunciations, dev_pronunciations, test_pronunciations = split_data(pronunciations, dev_proportion=.2, test_proportion=.01)\n",
    "len(train_pronunciations), len(dev_pronunciations), len(test_pronunciations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I split the corpus up into train/dev/test so that I could use the standard approach of training models with different hyperparameters and selecting the model that performs best against the dev set. Unfortunately that doesn't work well here because "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that I ran a larger parameter search before and saw that GRUs were consistently outperforming LSTMs and vanilla RNNs. There are 12 (4 * 3) models to train, and for each one I'm measuring train and dev error at every epoch. So if each model trains for the maximum of 2,000 epochs I would end up with 12 * 2,000 = 24,000 models to choose from. There's a good chance I'm overfitting the dev set with such a large search, but I'll inspect the learning curves to try to avoid selecting an iteration that randomly did well.\n",
    "\n",
    "While each model trains for a maximum of 2000 epochs, it stops early if the dev error does not decrease for three epochs in a row. Since I'm going to be selecting the model with the lowest dev error there's no reason to keep training a model once it's started overfitting. Alternatively I could train all models to convergence and then add regularization to reduce the complexity and identify the sweet spot, but that's far more time consuming because it requires training more models and each of them for longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss: 3.3053\tdev loss: 3.3871                                                                                               \n",
      "\tGenerated: in train: 0%, assess: 0%, novel: 100%\n",
      "\t ˈ ʊ t æ ˌ l ɛ ɑː aɪ ʊ ɪ ŋ z ʌ ɪ ɝː b dʒ l eɪ s\n",
      "\t ɔɪ ˌ ʌ n ɝ eɪ uː ɪ iː ɛ ə m b j r ɔ ˌ iː n uː ʒ n m ʃ k v h aɪ ˌ\n",
      "\t ɝ ŋ\n",
      "\t iː ŋ w uː v ɛ iː s ʒ ɛ t ɝ j ɛ æ tʃ θ h\n",
      "\t ʒ s w\n",
      "Epoch 2: train loss: 2.8625\tdev loss: 3.0034                                                                                               \n",
      "\tGenerated: in train: 0%, assess: 0%, novel: 100%\n",
      "\t l j i i dʒ ɔɪ æ\n",
      "\t ŋ ɛ g w ɪ w\n",
      "\t ə ɛ z ˈ m w ə g oʊ ˈ\n",
      "\t ɝː eɪ aɪ l ˌ v ɛ aʊ\n",
      "\t ɝː dʒ ɛ b iː ɪ r ɔ aʊ g oʊ æ ˌ l\n",
      "Epoch 3: train loss: 2.4605\tdev loss: 2.6573                                                                                               \n",
      "\tGenerated: in train: 1%, assess: 0%, novel: 99%\n",
      "\t \n",
      "\t h s ɝ ə aʊ l iː iː h dʒ aʊ oʊ l ɝ iː ə h ɔɪ ʃ ˈ ɛ ʌ ʊ uː ʊ i ɑː\n",
      "\t z iː ʃ\n",
      "\t i ɝ ˈ ɔ ɝ z ŋ b aʊ f ˈ\n",
      "\t \n",
      "Epoch 4: train loss: 2.1054\tdev loss: 2.3507                                                                                               \n",
      "\tGenerated: in train: 1%, assess: 0%, novel: 99%\n",
      "\t ʌ b æ i s ʒ ɛ eɪ eɪ aʊ oʊ ˈ ə v s h h\n",
      "\t ˈ k aɪ\n",
      "\t ʊ iː ˈ ˌ ɔɪ ɝː eɪ z i l l m\n",
      "\t aʊ i w ŋ m ɑː tʃ s\n",
      "\t l ə n ə iː s\n",
      "Epoch 5: train loss: 1.8111\tdev loss: 2.0957                                                                                               \n",
      "\tGenerated: in train: 0%, assess: 0%, novel: 100%\n",
      "\t ˈ ŋ ɛ r ˌ ɛ\n",
      "\t ʌ aʊ f f r ʌ\n",
      "\t aʊ ʃ i ɛ ə uː t aʊ eɪ\n",
      "\t ɝː ˌ i eɪ ə uː ˌ i ʌ ʊ d ˈ s\n",
      "\t ɔɪ l\n",
      "Epoch 6: train loss: 1.5803\tdev loss: 1.8934                                                                                               \n",
      "\tGenerated: in train: 1%, assess: 0%, novel: 99%\n",
      "\t ʃ w θ f\n",
      "\t g iː i i ɝ l\n",
      "\t \n",
      "\t ʊ\n",
      "\t w l ɛ h ɛ iː v ʌ j g iː r ˌ ə ʌ i æ aʊ ɑː oʊ ð b\n",
      "Epoch 7: train loss: 1.4051\tdev loss: 1.7377                                                                                               \n",
      "\tGenerated: in train: 1%, assess: 0%, novel: 99%\n",
      "\t aɪ z ʌ ˈ\n",
      "\t s j\n",
      "\t \n",
      "\t ɔɪ ɔɪ\n",
      "\t ˈ ɑː iː aʊ ʌ ˈ aʊ j j k\n",
      "Epoch 8: train loss: 1.2749\tdev loss: 1.6196                                                                                               \n",
      "\tGenerated: in train: 1%, assess: 0%, novel: 99%\n",
      "\t ʒ v b b eɪ ˈ l eɪ\n",
      "\t r ə w l eɪ i aʊ uː\n",
      "\t ɛ ˈ l\n",
      "\t k ɪ h aʊ ɝː t h b r\n",
      "\t ɝ ʒ k ɛ ʃ\n",
      "Epoch 9: train loss: 1.1789\tdev loss: 1.5302                                                                                               \n",
      "\tGenerated: in train: 0%, assess: 0%, novel: 100%\n",
      "\t ʊ aʊ ŋ tʃ g ɑː ɛ oʊ ˌ n uː eɪ ɔ r l i g d\n",
      "\t ɔɪ aʊ aɪ aɪ ŋ m m l ə\n",
      "\t h ɛ ˌ j ŋ ˈ iː s aʊ g ɝ ɪ ˌ eɪ ˌ dʒ ŋ i eɪ\n",
      "\t p ɝː r\n",
      "\t z aʊ ˈ iː p ʃ aʊ eɪ ɔ ɔɪ\n",
      "Epoch 10: train loss: 1.1054\tdev loss: 1.4601                                                                                               \n",
      "\tGenerated: in train: 1%, assess: 0%, novel: 99%\n",
      "\t ɪ ˈ t ə p ɔ aʊ ˌ\n",
      "\t v n ɔ b j l ˌ z p r l d\n",
      "\t i ə\n",
      "\t f z ʒ aʊ b ʒ k θ b oʊ oʊ ˈ r eɪ uː aʊ\n",
      "\t ˈ ˈ w n ˈ ɛ k ɔ i ɛ iː iː\n",
      "Epoch 11: train loss: 1.0488\tdev loss: 1.4049                                                                                               \n",
      "\tGenerated: in train: 0%, assess: 0%, novel: 100%\n",
      "\t r l ŋ ˈ\n",
      "\t θ ˈ ɑː l ð\n",
      "\t ə eɪ ɛ p z\n",
      "\t ŋ ɝː p i ˌ\n",
      "\t tʃ ə t b ə θ ɝː\n",
      "Epoch 12: train loss: 1.0044\tdev loss: 1.3610                                                                                               \n",
      "\tGenerated: in train: 1%, assess: 0%, novel: 99%\n",
      "\t \n",
      "\t k k ɝ j k b ɔ\n",
      "\t b t ə tʃ ʌ iː eɪ\n",
      "\t g\n",
      "\t aʊ i ˌ æ iː ʃ iː k aɪ ɝ\n",
      "Epoch 13: train loss: 0.9690\tdev loss: 1.3253                                                                                               \n",
      "\tGenerated: in train: 1%, assess: 0%, novel: 99%\n",
      "\t eɪ ˌ ˈ\n",
      "\t iː ə æ n r ˌ ˌ l ˌ p b iː\n",
      "\t ˈ ə\n",
      "\t j ˈ g\n",
      "\t ˈ b ɑː ˈ ŋ\n",
      "Epoch 14: train loss: 0.9405\tdev loss: 1.2963                                                                                               \n",
      "\tGenerated: in train: 4%, assess: 0%, novel: 96%\n",
      "\t ˈ g w\n",
      "\t ˈ æ i k\n",
      "\t j ɪ ˈ l l ɝ ˌ ʃ g s n ɪ\n",
      "\t l ɑː ə n iː ɪ ˈ i\n",
      "\t f ŋ ˈ n r\n",
      "Epoch 15: train loss: 0.9174\tdev loss: 1.2725                                                                                               \n",
      "\tGenerated: in train: 1%, assess: 0%, novel: 99%\n",
      "\t ˈ h ˈ ɑː\n",
      "\t ˈ ə w æ\n",
      "\t ɪ ˈ ɝ eɪ aʊ ˈ r\n",
      "\t z r r ɝː k b ˈ ˈ t\n",
      "\t ɛ ˈ g oʊ ˌ\n",
      "Epoch 16: train loss: 0.8983\tdev loss: 1.2527                                                                                               \n",
      "\tGenerated: in train: 2%, assess: 0%, novel: 98%\n",
      "\t ˈ ˈ n\n",
      "\t ˌ ˈ ˈ p s l\n",
      "\t r n dʒ r eɪ ɛ f ɝː ɝ ˈ s\n",
      "\t iː θ f p eɪ b ˈ\n",
      "\t \n",
      "Epoch 17: train loss: 0.8817\tdev loss: 1.2354                                                                                               \n",
      "\tGenerated: in train: 2%, assess: 0%, novel: 98%\n",
      "\t j ˈ ŋ d n n z ɪ\n",
      "\t ˈ r aɪ\n",
      "\t ˈ ɪ ˌ v æ t\n",
      "\t n ɝ l g ˈ ɪ eɪ r ɔɪ ɝ\n",
      "\t g ˈ t\n",
      "Epoch 18: train loss: 0.8680\tdev loss: 1.2208                                                                                               \n",
      "\tGenerated: in train: 1%, assess: 0%, novel: 99%\n",
      "\t ɑː ˈ aʊ ŋ b\n",
      "\t ˈ ŋ ə\n",
      "\t ˈ ˈ v ˈ\n",
      "\t n ˌ uː r g iː ˈ v ɝ t r ə\n",
      "\t oʊ ˈ n ɔ ɝ eɪ\n",
      "Epoch 19: train loss: 0.8561\tdev loss: 1.2081                                                                                               \n",
      "\tGenerated: in train: 3%, assess: 0%, novel: 97%\n",
      "\t ɔɪ r aɪ ɔ ˈ z ɛ ɪ w\n",
      "\t z s ʊ t ˈ v ˈ p t\n",
      "\t k ˈ æ ˈ\n",
      "\t ʌ g ˌ ə w\n",
      "\t ˌ aʊ ˌ ʌ ŋ m\n",
      "Epoch 20: train loss: 0.8460\tdev loss: 1.1971                                                                                               \n",
      "\tGenerated: in train: 1%, assess: 2%, novel: 97%\n",
      "\t ʃ ˈ ŋ ɛ ɪ r z\n",
      "\t k ɪ z ɑː f ˈ\n",
      "\t ˈ ˈ\n",
      "\t eɪ ˈ r aɪ ɪ z\n",
      "\t ɔ dʒ b ˈ l ʊ r\n",
      "Epoch 21: train loss: 0.8368\tdev loss: 1.1871                                                                                               \n",
      "\tGenerated: in train: 3%, assess: 0%, novel: 97%\n",
      "\t ɔɪ ʃ ˈ\n",
      "\t l r uː ˈ ɛ ˈ\n",
      "\t eɪ iː ŋ ˈ ʌ p eɪ g m l ˌ v\n",
      "\t r ˈ iː t ˌ ɪ ɛ\n",
      "\t d l ˈ æ r\n",
      "Epoch 22: train loss: 0.8287\tdev loss: 1.1782                                                                                               \n",
      "\tGenerated: in train: 1%, assess: 0%, novel: 99%\n",
      "\t dʒ ɪ ɝ ˈ ʌ ɛ ɪ ˌ ʃ\n",
      "\t ŋ ˈ k\n",
      "\t w ˈ ˈ\n",
      "\t ˈ k ˌ ɪ aɪ dʒ\n",
      "\t ˈ b s ŋ\n",
      "Epoch 23: train loss: 0.8215\tdev loss: 1.1701                                                                                               \n",
      "\tGenerated: in train: 1%, assess: 0%, novel: 99%\n",
      "\t ˈ m d ɑː æ p ʌ ɪ ˌ ð k ɪ ɑː ɑː ɪ æ ŋ ɪ\n",
      "\t ˈ n\n",
      "\t ˈ æ t ɪ b ə\n",
      "\t s ˈ n\n",
      "\t ɪ ˈ i m ɛ\n",
      "Epoch 24: train loss: 0.8146\tdev loss: 1.1625                                                                                               \n",
      "\tGenerated: in train: 2%, assess: 0%, novel: 98%\n",
      "\t k p k l ˌ i\n",
      "\t ˈ ɑː\n",
      "\t m ˈ ʃ ˈ ˌ iː ə i\n",
      "\t iː m t l iː ɝː ə ð ˈ m uː uː ŋ\n",
      "\t ˈ ɛ s ə n ʌ h iː ɪ\n",
      "Epoch 25: train loss: 0.8089\tdev loss: 1.1561                                                                                               \n",
      "\tGenerated: in train: 3%, assess: 0%, novel: 97%\n",
      "\t eɪ ˈ ɔɪ eɪ h ɔ g m r n\n",
      "\t ˈ ŋ ə ˈ\n",
      "\t ˈ ɑː ˈ\n",
      "\t eɪ r ˈ l\n",
      "\t ˈ iː n s b v ə d s\n",
      "Epoch 26: train loss: 0.8035\tdev loss: 1.1500                                                                                               \n",
      "\tGenerated: in train: 2%, assess: 0%, novel: 98%\n",
      "\t ˈ ɛ ˈ p t ɝ s\n",
      "\t ˈ i\n",
      "\t \n",
      "\t l ˈ ɪ oʊ\n",
      "\t t ˈ t ŋ l ʌ\n",
      "Epoch 27: train loss: 0.7987\tdev loss: 1.1445                                                                                               \n",
      "\tGenerated: in train: 2%, assess: 0%, novel: 98%\n",
      "\t f æ oʊ ɑː ˈ j r ɛ t\n",
      "\t ˈ ɪ g ˌ ɪ ɛ k iː z z r ə\n",
      "\t f eɪ t p d ˈ k\n",
      "\t d ˈ d k iː uː\n",
      "\t ˈ\n",
      "Epoch 28: train loss: 0.7947\tdev loss: 1.1400                                                                                               \n",
      "\tGenerated: in train: 0%, assess: 0%, novel: 100%\n",
      "\t ˈ\n",
      "\t ɛ ʌ k ˈ\n",
      "\t ʊ ˈ æ\n",
      "\t m ɪ ˈ ɑː n s ɛ l ˌ ˈ n\n",
      "\t ˈ p oʊ p ɔ ɪ k ɔɪ æ t ɪ\n",
      "Epoch 29: train loss: 0.7904\tdev loss: 1.1352                                                                                               \n",
      "\tGenerated: in train: 1%, assess: 1%, novel: 98%\n",
      "\t g t t ˈ ˈ d z aɪ\n",
      "\t l ˈ t ˈ\n",
      "\t ˈ æ ŋ\n",
      "\t ˈ k b i\n",
      "\t ŋ ˈ æ n tʃ\n",
      "Epoch 30: train loss: 0.7868\tdev loss: 1.1309                                                                                               \n",
      "\tGenerated: in train: 1%, assess: 1%, novel: 98%\n",
      "\t aʊ ˈ aɪ\n",
      "\t ə ˈ ˈ ɛ\n",
      "\t ˈ iː b ʌ eɪ z ə\n",
      "\t w uː ˈ iː ˈ t oʊ ɛ\n",
      "\t k ˈ ɪ k æ l l ɝ d ʌ n l n l ɔɪ s ɑː k\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31: train loss: 0.7836\tdev loss: 1.1273                                                                                               \n",
      "\tGenerated: in train: 1%, assess: 0%, novel: 99%\n",
      "\t ˈ ˈ ɪ\n",
      "\t ˈ ə ˈ t\n",
      "\t r ˈ ɝ aɪ\n",
      "\t j ˈ ɪ ə ɔ l uː ə ə f s s ˈ g\n",
      "\t r ˈ uː k d\n",
      "Epoch 32: train loss: 0.7806\tdev loss: 1.1238                                                                                               \n",
      "\tGenerated: in train: 0%, assess: 0%, novel: 100%\n",
      "\t ˈ p\n",
      "\t aʊ ˈ i ɛ s t r\n",
      "\t ˈ ˈ\n",
      "\t ˈ ˈ i\n",
      "\t ˈ s b b k m s\n",
      "Epoch 33: train loss: 0.7778\tdev loss: 1.1205                                                                                               \n",
      "\tGenerated: in train: 2%, assess: 0%, novel: 98%\n",
      "\t v uː n ˈ t l ɛ ɛ ˌ ˌ f ɑː ˈ\n",
      "\t ˈ n n\n",
      "\t r ˈ t n\n",
      "\t ʃ ˈ b ʃ iː v\n",
      "\t n ˈ ɪ k t iː n r\n",
      "Epoch 34: train loss: 0.7752\tdev loss: 1.1176                                                                                               \n",
      "\tGenerated: in train: 3%, assess: 0%, novel: 97%\n",
      "\t ˈ ə ə t r s h ˈ\n",
      "\t ʌ ˈ z k ʌ m ɛ\n",
      "\t s ˈ m r æ\n",
      "\t ɛ ˈ ɪ l n\n",
      "\t ˈ ˌ w ˈ\n",
      "Epoch 35: train loss: 0.7730\tdev loss: 1.1150                                                                                               \n",
      "\tGenerated: in train: 4%, assess: 0%, novel: 96%\n",
      "\t ˈ iː ə\n",
      "\t ɔ r ˈ k ŋ ɔ g ɝ\n",
      "\t r ˈ l ˈ æ n b ɪ\n",
      "\t r ˌ ɪ n ˈ ə d v t l z\n",
      "\t s ˈ ɪ θ s r l\n",
      "Epoch 36: train loss: 0.7709\tdev loss: 1.1125                                                                                               \n",
      "\tGenerated: in train: 1%, assess: 0%, novel: 99%\n",
      "\t ˈ ˈ l s\n",
      "\t ˈ dʒ w n ə\n",
      "\t ˈ ɛ eɪ v\n",
      "\t ɪ ˈ ə ˈ d m oʊ l ˌ k l æ b j\n",
      "\t s r aʊ ˈ m r m\n",
      "Epoch 37: train loss: 0.7680\tdev loss: 1.1089                                                                                               \n",
      "\tGenerated: in train: 3%, assess: 1%, novel: 96%\n",
      "\t t ɑː ɑː n ˈ r l t\n",
      "\t ˈ s eɪ oʊ m z b\n",
      "\t ˈ\n",
      "\t ˈ m d g g ɑː\n",
      "\t ə d aɪ k ˈ oʊ h z v l uː ˌ\n",
      "Epoch 38: train loss: 0.7658\tdev loss: 1.1064                                                                                               \n",
      "\tGenerated: in train: 0%, assess: 0%, novel: 100%\n",
      "\t n d h p ˈ ə ɛ t d\n",
      "\t ə ɪ ˈ i\n",
      "\t ˈ r\n",
      "\t b k r aʊ ɛ ˈ n z\n",
      "\t ɪ iː ˈ b aɪ\n",
      "Epoch 39: train loss: 0.7642\tdev loss: 1.1044                                                                                               \n",
      "\tGenerated: in train: 1%, assess: 0%, novel: 99%\n",
      "\t l θ ˈ r s s\n",
      "\t ˌ v n\n",
      "\t ˈ ɑː ˈ ɔ oʊ r\n",
      "\t ˈ ˈ n oʊ\n",
      "\t ˈ\n",
      "Epoch 40: train loss: 0.7622\tdev loss: 1.1020                                                                                               \n",
      "\tGenerated: in train: 1%, assess: 1%, novel: 98%\n",
      "\t ˌ p l ɛ ɔɪ ˈ tʃ h ɛ d\n",
      "\t d æ ˈ h iː ʃ tʃ oʊ d\n",
      "\t ˈ ˈ\n",
      "\t ˈ ʌ oʊ l r r r\n",
      "\t g ˈ ə t r ɪ n\n",
      "Epoch 41: train loss: 0.7609\tdev loss: 1.1004                                                                                               \n",
      "\tGenerated: in train: 2%, assess: 0%, novel: 98%\n",
      "\t ˌ t s ˌ ˈ i d\n",
      "\t ɑː ˈ ɔɪ ˌ iː d s ˌ k k d s\n",
      "\t k aɪ ˈ iː g l s ɔ s n s\n",
      "\t k oʊ ˈ s θ ˌ ɑː aɪ ɔɪ\n",
      "\t n ˈ ˈ ɛ æ ə k uː ɝ\n",
      "Epoch 42: train loss: 0.7592\tdev loss: 1.0985                                                                                               \n",
      "\tGenerated: in train: 0%, assess: 1%, novel: 99%\n",
      "\t ˈ ɛ d ɑː n w v\n",
      "\t ɝ r p l b v ˌ ˈ\n",
      "\t ˈ l ʃ ə ɪ æ l ə uː\n",
      "\t ɛ ˈ ɝ aɪ z\n",
      "\t ˌ ˈ æ r z r ɝ z\n",
      "Epoch 43: train loss: 0.7578\tdev loss: 1.0967                                                                                               \n",
      "\tGenerated: in train: 1%, assess: 0%, novel: 99%\n",
      "\t r k eɪ v ˈ ɝ ɑː ə ɪ\n",
      "\t ɑː ˈ s ɪ n ˌ eɪ k\n",
      "\t l iː ˈ\n",
      "\t ˈ s\n",
      "\t iː ˌ ˈ\n",
      "Epoch 44: train loss: 0.7565\tdev loss: 1.0951                                                                                               \n",
      "\tGenerated: in train: 3%, assess: 0%, novel: 97%\n",
      "\t s s ˈ ə n b p ɑː ŋ ə l ɪ\n",
      "\t ˈ w ˈ ə\n",
      "\t p ˈ oʊ p s ɪ\n",
      "\t ˈ ɪ ɔ m k l t s\n",
      "\t ˌ k ʌ d oʊ iː ʃ l\n",
      "Epoch 45: train loss: 0.7545\tdev loss: 1.0928                                                                                               \n",
      "\tGenerated: in train: 3%, assess: 0%, novel: 97%\n",
      "\t ˈ uː ɛ p k\n",
      "\t ˈ ə k iː ʃ n d\n",
      "\t r j ˈ i\n",
      "\t ˌ m ˌ ɛ n ˈ ˈ s ɝ\n",
      "\t ˈ r s ɪ ə s t b ə n l ɝ\n",
      "Epoch 46: train loss: 0.7534\tdev loss: 1.0913                                                                                               \n",
      "\tGenerated: in train: 2%, assess: 0%, novel: 98%\n",
      "\t ˈ k uː r ŋ g d\n",
      "\t ˈ ˌ n n d t s\n",
      "\t ˈ oʊ n ˈ h\n",
      "\t b ɝ n ˈ j p r t b ŋ\n",
      "\t ɪ ˈ eɪ\n",
      "Epoch 47: train loss: 0.7517\tdev loss: 1.0893                                                                                               \n",
      "\tGenerated: in train: 3%, assess: 1%, novel: 96%\n",
      "\t l ə ˈ k k\n",
      "\t ˈ eɪ ˌ ɪ ɪ n r\n",
      "\t ˈ i\n",
      "\t ʌ ˈ ɪ s ə ə ɔ b i\n",
      "\t ˈ ɛ l\n",
      "Epoch 48: train loss: 0.7506\tdev loss: 1.0879                                                                                               \n",
      "\tGenerated: in train: 4%, assess: 0%, novel: 96%\n",
      "\t ˈ oʊ ə ə t b eɪ\n",
      "\t w ˈ oʊ l j\n",
      "\t ˈ eɪ\n",
      "\t ɛ r t p ˈ ɛ z h h v\n",
      "\t ˈ l ɝː iː m n iː\n",
      "Epoch 49: train loss: 0.7497\tdev loss: 1.0866                                                                                               \n",
      "\tGenerated: in train: 2%, assess: 0%, novel: 98%\n",
      "\t t ˌ m ə m ɛ ɔɪ ˈ l ɛ r ˌ r ɪ\n",
      "\t ˈ r ɑː m\n",
      "\t iː ˌ ə ˈ l ˌ f z n ɔ ə z\n",
      "\t ˈ r ɔ ɑː ɪ m ɑː æ ɝ\n",
      "\t ˈ dʒ iː n m\n",
      "Epoch 50: train loss: 0.7486\tdev loss: 1.0853                                                                                               \n",
      "\tGenerated: in train: 1%, assess: 0%, novel: 99%\n",
      "\t ŋ t æ ˈ n k l\n",
      "\t aʊ ˈ v ˈ n t ɝ ʃ\n",
      "\t d ˌ n t ˈ ɪ b s ˌ ə t t\n",
      "\t g n ˈ p\n",
      "\t ˈ\n",
      "Epoch 51: train loss: 0.7472\tdev loss: 1.0836                                                                                               \n",
      "\tGenerated: in train: 0%, assess: 0%, novel: 100%\n",
      "\t ˌ æ ˈ r n\n",
      "\t ə ˈ ɛ b p n tʃ\n",
      "\t ɑː k ɑː ˈ t ɪ\n",
      "\t ˈ ə ɛ oʊ p g r ɝː ɝ ə\n",
      "\t ˈ ə\n",
      "Epoch 52: train loss: 0.7460\tdev loss: 1.0821                                                                                               \n",
      "\tGenerated: in train: 1%, assess: 1%, novel: 98%\n",
      "\t ˈ f uː s b æ oʊ ʃ\n",
      "\t ˌ r ˈ ʃ ŋ oʊ w\n",
      "\t ˈ\n",
      "\t ˈ k k n ŋ ɪ g æ ŋ n t t ɛ k ɝ z\n",
      "\t ɑː ˈ w ɝ ɛ\n",
      "Epoch 53: train loss: 0.7452\tdev loss: 1.0809                                                                                               \n",
      "\tGenerated: in train: 0%, assess: 0%, novel: 100%\n",
      "\t ˌ ˈ m ə l i z\n",
      "\t n ˈ l j ɪ\n",
      "\t ˈ r ˌ z r ɪ s ɪ ə r ɔɪ t z k b z\n",
      "\t p ˈ n\n",
      "\t ˈ n ɪ oʊ\n",
      "Epoch 54: train loss: 0.7442\tdev loss: 1.0795                                                                                               \n",
      "\tGenerated: in train: 2%, assess: 0%, novel: 98%\n",
      "\t ˈ d ð\n",
      "\t f m ʌ ˈ d d d aɪ s iː\n",
      "\t ˈ s m r ə r d f\n",
      "\t g ˈ aʊ\n",
      "\t r ɔ v ˌ t æ ˈ w æ t\n",
      "Epoch 55: train loss: 0.7432\tdev loss: 1.0782                                                                                               \n",
      "\tGenerated: in train: 2%, assess: 0%, novel: 98%\n",
      "\t s l n m r ˈ l\n",
      "\t ɛ iː ˈ ʒ ŋ r oʊ ə ŋ n s ə n\n",
      "\t ˈ iː r s p m g\n",
      "\t ˈ b v l k ʃ t\n",
      "\t r n ɪ aɪ f ɑː r h ˈ m ə n k\n",
      "Epoch 56: train loss: 0.7418\tdev loss: 1.0766                                                                                               \n",
      "\tGenerated: in train: 4%, assess: 0%, novel: 96%\n",
      "\t l ˈ ˌ k m k l n\n",
      "\t ˈ\n",
      "\t ˈ uː dʒ t ʒ ɪ uː ʒ z ə\n",
      "\t ˈ k aʊ\n",
      "\t ˈ ɪ t b k ʃ\n",
      "Epoch 57: train loss: 0.7412\tdev loss: 1.0757                                                                                               \n",
      "\tGenerated: in train: 2%, assess: 0%, novel: 98%\n",
      "\t g ˈ eɪ ɝ\n",
      "\t tʃ ˈ ˌ ɪ r k\n",
      "\t ɛ ˈ t n ɪ aɪ ɑː r t ɛ\n",
      "\t ˈ ɑː ˈ ə\n",
      "\t ɛ g ˈ n oʊ r b z d ɝ ə\n",
      "Epoch 58: train loss: 0.7398\tdev loss: 1.0739                                                                                               \n",
      "\tGenerated: in train: 3%, assess: 1%, novel: 96%\n",
      "\t l n ˈ n ˌ ˌ ɪ æ oʊ iː r s aɪ\n",
      "\t ˈ ˈ\n",
      "\t ˈ\n",
      "\t ˈ dʒ ʌ r s oʊ ə ˌ ə ɪ g ɝ iː\n",
      "\t ˈ ɪ l t l v\n",
      "Epoch 59: train loss: 0.7388\tdev loss: 1.0725                                                                                               \n",
      "\tGenerated: in train: 2%, assess: 0%, novel: 98%\n",
      "\t ˈ l\n",
      "\t ˈ ɛ ˌ p aɪ ɝ\n",
      "\t ɛ oʊ ˈ ɔ p ɪ t m w p ə æ z t g l\n",
      "\t ˈ n t n k z l\n",
      "\t ˈ ˈ\n",
      "Epoch 60: train loss: 0.7380\tdev loss: 1.0712                                                                                               \n",
      "\tGenerated: in train: 4%, assess: 0%, novel: 96%\n",
      "\t l ˈ m l z k\n",
      "\t ˈ t ˈ ɪ\n",
      "\t ˈ l n oʊ z n d z\n",
      "\t ə ɪ b ˈ iː l l\n",
      "\t r ˈ s v ə s r æ\n",
      "Epoch 61: train loss: 0.7369\tdev loss: 1.0699                                                                                               \n",
      "\tGenerated: in train: 1%, assess: 0%, novel: 99%\n",
      "\t ˈ k l ə ə\n",
      "\t ˈ s v æ n m iː s\n",
      "\t ˈ g ŋ n ə g ɝ\n",
      "\t r ˈ l t\n",
      "\t b dʒ ˈ s k ɔ n v l\n",
      "Epoch 62: train loss: 0.7358\tdev loss: 1.0684                                                                                               \n",
      "\tGenerated: in train: 3%, assess: 0%, novel: 97%\n",
      "\t l v ɑː ˈ m z aʊ m\n",
      "\t p ˈ ɔɪ s dʒ\n",
      "\t b ˈ s s k uː ɪ r n æ ŋ ˌ uː l iː t z\n",
      "\t ˈ k t w\n",
      "\t ə ˈ\n",
      "Epoch 63: train loss: 0.7347\tdev loss: 1.0668                                                                                               \n",
      "\tGenerated: in train: 2%, assess: 0%, novel: 98%\n",
      "\t t ˈ ɪ g aɪ m t\n",
      "\t ɪ iː ˈ ɪ iː n l k\n",
      "\t iː ə ɪ ɪ d ˈ aɪ g\n",
      "\t ɑː ˈ n n n\n",
      "\t ˈ ˈ\n",
      "Epoch 64: train loss: 0.7333\tdev loss: 1.0651                                                                                               \n",
      "\tGenerated: in train: 1%, assess: 0%, novel: 99%\n",
      "\t ˈ s dʒ d æ eɪ\n",
      "\t n n ʃ ˈ k ɔɪ k ʃ s ˌ p ə b z n p\n",
      "\t ˈ v dʒ l i\n",
      "\t t ˈ l d r n ə\n",
      "\t ˈ j r\n",
      "Epoch 65: train loss: 0.7323\tdev loss: 1.0638                                                                                               \n",
      "\tGenerated: in train: 2%, assess: 1%, novel: 97%\n",
      "\t ˈ h w b\n",
      "\t ˈ s æ i n z\n",
      "\t ə r r ˈ uː ə ə k p b ə\n",
      "\t ˈ ˈ f z\n",
      "\t ˈ oʊ ʃ ɪ ə eɪ ɪ t ŋ l\n",
      "Epoch 66: train loss: 0.7313\tdev loss: 1.0623                                                                                               \n",
      "\tGenerated: in train: 2%, assess: 1%, novel: 97%\n",
      "\t ɪ ˌ ˈ b i\n",
      "\t ˈ r v oʊ g\n",
      "\t ŋ ˈ ˌ t s\n",
      "\t ˈ s f n iː k aɪ n t\n",
      "\t ˌ ˈ\n",
      "Epoch 67: train loss: 0.7302\tdev loss: 1.0607                                                                                               \n",
      "\tGenerated: in train: 3%, assess: 0%, novel: 97%\n",
      "\t ˈ ɔɪ oʊ iː l t\n",
      "\t ɑː ˈ p s b w n m\n",
      "\t t ɛ l æ ɔ l n n oʊ t ˈ aʊ i ə\n",
      "\t ˈ d ə ɑː oʊ ɝ\n",
      "\t n ˈ ʌ tʃ v k ɪ\n",
      "Epoch 68: train loss: 0.7280\tdev loss: 1.0576                                                                                               \n",
      "\tGenerated: in train: 1%, assess: 1%, novel: 98%\n",
      "\t ˈ p ə m b\n",
      "\t ˈ\n",
      "\t ʃ ˈ\n",
      "\t ˈ d ɪ r ɔ n\n",
      "\t ˈ dʒ k ɔ k ɪ l\n",
      "Epoch 69: train loss: 0.7270\tdev loss: 1.0562                                                                                               \n",
      "\tGenerated: in train: 3%, assess: 0%, novel: 97%\n",
      "\t ˈ iː t s ə eɪ k\n",
      "\t ɑː ˈ l ŋ b m\n",
      "\t iː ɪ ˈ ɑː l v ə\n",
      "\t ˈ w ˌ n r b\n",
      "\t n s ˈ h r\n",
      "Epoch 70: train loss: 0.7262\tdev loss: 1.0551                                                                                               \n",
      "\tGenerated: in train: 0%, assess: 0%, novel: 100%\n",
      "\t ˈ æ r k ɛ t m\n",
      "\t ˈ h s s ɪ k l l ɑː ŋ t tʃ\n",
      "\t ʃ ˈ ə ɝː l ɪ n\n",
      "\t ˈ\n",
      "\t ɔ l ˈ ɔɪ ɪ\n",
      "Epoch 71: train loss: 0.7251\tdev loss: 1.0534                                                                                               \n",
      "\tGenerated: in train: 5%, assess: 1%, novel: 94%\n",
      "\t ˈ d ˈ aɪ æ r f æ s\n",
      "\t ɝ oʊ ə ˈ oʊ ɔ ə ˈ\n",
      "\t r b d ˈ w v ɪ s ŋ ə\n",
      "\t ˈ k aɪ s n ə iː ɝ iː\n",
      "\t t ˈ l ə l t θ\n",
      "Epoch 72: train loss: 0.7244\tdev loss: 1.0525                                                                                               \n",
      "\tGenerated: in train: 1%, assess: 0%, novel: 99%\n",
      "\t ˈ ə ə ɑː t m oʊ\n",
      "\t ˈ aɪ n m\n",
      "\t ˈ ɛ s ə ˌ ɛ ə ɪ s aɪ\n",
      "\t m ˈ ʃ d ɪ l\n",
      "\t ˈ f b p i\n",
      "Epoch 73: train loss: 0.7235\tdev loss: 1.0512                                                                                               \n",
      "\tGenerated: in train: 1%, assess: 0%, novel: 99%\n",
      "\t ɪ d n l ˈ n æ ə d s n\n",
      "\t ˈ eɪ ɪ ɝ\n",
      "\t ˈ iː dʒ k oʊ æ ɝ\n",
      "\t ˈ w\n",
      "\t ˈ aɪ ɝ ə\n",
      "Epoch 74: train loss: 0.7227\tdev loss: 1.0501                                                                                               \n",
      "\tGenerated: in train: 1%, assess: 0%, novel: 99%\n",
      "\t p r ˈ p t ɪ eɪ eɪ ə\n",
      "\t ˈ v ɛ l ə m z\n",
      "\t r ˈ s æ p t ə z\n",
      "\t ˈ eɪ j ə r z b r ɪ m ŋ i k n\n",
      "\t f ˈ ˈ\n",
      "Epoch 75: train loss: 0.7221\tdev loss: 1.0491                                                                                               \n",
      "\tGenerated: in train: 2%, assess: 0%, novel: 98%\n",
      "\t p ˈ ə k s eɪ ɪ n\n",
      "\t ˈ ɔ j aɪ ˌ s w p d n\n",
      "\t ˈ ɔ æ ə ə d oʊ r\n",
      "\t iː ˈ\n",
      "\t ˌ tʃ ˈ iː n iː ŋ\n",
      "Epoch 76: train loss: 0.7210\tdev loss: 1.0478                                                                                               \n",
      "\tGenerated: in train: 2%, assess: 0%, novel: 98%\n",
      "\t ˈ w aʊ n z ŋ\n",
      "\t ˈ dʒ ɪ ˈ\n",
      "\t ʌ ˌ f ˈ b ɪ\n",
      "\t n ɑː ˈ ɪ æ ə ˌ g z ɝ\n",
      "\t k ˌ b p g ˈ ɛ æ uː aɪ l ʌ d\n",
      "Epoch 77: train loss: 0.7202\tdev loss: 1.0466                                                                                               \n",
      "\tGenerated: in train: 2%, assess: 0%, novel: 98%\n",
      "\t k ˈ w\n",
      "\t ˈ v f b l m l\n",
      "\t ˈ ɪ ɪ n t z\n",
      "\t p ˈ ɑː ɔ d ɪ w ˌ g oʊ r m n n\n",
      "\t ˈ ə m eɪ ɝ\n",
      "Epoch 78: train loss: 0.7189\tdev loss: 1.0450                                                                                               \n",
      "\tGenerated: in train: 3%, assess: 0%, novel: 97%\n",
      "\t ˈ ˈ ˌ ə ɪ tʃ ŋ n z\n",
      "\t ɔ v l r ˈ k p i ŋ\n",
      "\t k ˌ k ˈ ɛ\n",
      "\t ˈ ɛ t iː n r ə z ɛ ŋ t l oʊ\n",
      "\t ˈ l ə ɪ ɪ t\n",
      "Epoch 79: train loss: 0.7178\tdev loss: 1.0433                                                                                               \n",
      "\tGenerated: in train: 1%, assess: 0%, novel: 99%\n",
      "\t j r ˈ ə ɝ ɑː\n",
      "\t ˈ m r p ɔ r ʌ\n",
      "\t iː ɛ ˈ k ˌ v ɔ r s r\n",
      "\t ˈ r r ɛ iː ʌ ɪ ɝ g\n",
      "\t ə ˈ f g æ iː\n",
      "Epoch 80: train loss: 0.7172\tdev loss: 1.0425                                                                                               \n",
      "\tGenerated: in train: 1%, assess: 0%, novel: 99%\n",
      "\t ˈ r f m r z t ɪ ˌ ɪ ə n i\n",
      "\t ˈ ɔ æ r ə ə\n",
      "\t ˈ l d l ɝ\n",
      "\t w k d ˈ g r s g oʊ\n",
      "\t l s t ˌ h ʒ m ˈ iː h b n\n",
      "Epoch 81: train loss: 0.7159\tdev loss: 1.0410                                                                                               \n",
      "\tGenerated: in train: 1%, assess: 0%, novel: 99%\n",
      "\t æ ˈ p t r l i\n",
      "\t ˈ m n l ɪ ə v ə t z d\n",
      "\t ˈ s ɝː s n\n",
      "\t ˈ v l l æ n iː n z\n",
      "\t ˈ ɪ l ə k ə d n\n",
      "Epoch 82: train loss: 0.7154\tdev loss: 1.0403                                                                                               \n",
      "\tGenerated: in train: 2%, assess: 0%, novel: 98%\n",
      "\t ˈ ʃ k p ɛ d\n",
      "\t d ˈ eɪ d t i l ɝ n\n",
      "\t ˈ oʊ r θ s t w l k ə g s ə k z\n",
      "\t ˈ l d p b ə ə r\n",
      "\t iː ˈ ʒ ə m f\n",
      "Epoch 83: train loss: 0.7149\tdev loss: 1.0395                                                                                               \n",
      "\tGenerated: in train: 1%, assess: 0%, novel: 99%\n",
      "\t ˈ l m s n oʊ\n",
      "\t ˈ ɪ n\n",
      "\t ˈ oʊ b r ɛ ə t t\n",
      "\t r ˈ ə f ɪ l ə t\n",
      "\t ˈ ɛ ˈ eɪ d\n",
      "Epoch 84: train loss: 0.7143\tdev loss: 1.0386                                                                                               \n",
      "\tGenerated: in train: 0%, assess: 0%, novel: 100%\n",
      "\t ə ˈ k z j l\n",
      "\t p r ˈ r ð s ɝː\n",
      "\t ˈ d ə\n",
      "\t r v n ˈ\n",
      "\t n f ɛ ˈ r b ɛ ɝː ɝ\n",
      "Epoch 85: train loss: 0.7141\tdev loss: 1.0382                                                                                               \n",
      "\tGenerated: in train: 1%, assess: 0%, novel: 99%\n",
      "\t ˈ ɪ tʃ k k\n",
      "\t ɑː p b ˈ aɪ ɝ ə uː k t ɪ k\n",
      "\t r ˈ w ɛ ɪ t ɪ ə ə oʊ z eɪ\n",
      "\t ˈ ɛ n ɝː ʃ t n\n",
      "\t ˈ d n ɛ t ɑː l\n",
      "Epoch 86: train loss: 0.7134\tdev loss: 1.0376                                                                                               \n",
      "\tGenerated: in train: 3%, assess: 0%, novel: 97%\n",
      "\t t ˈ ˈ s\n",
      "\t ˈ p l t n\n",
      "\t ˈ aʊ dʒ s s ə æ k b ɪ ʃ t\n",
      "\t p ˈ iː ɪ ɑː t ˌ d g ɪ n\n",
      "\t ˌ ˌ s iː ɪ ˌ n n ɔ ɛ ɝ ɝ ə d ɪ ɪ k ˌ s ɪ ɛ d l t k n\n",
      "Epoch 87: train loss: 0.7128\tdev loss: 1.0366                                                                                               \n",
      "\tGenerated: in train: 6%, assess: 0%, novel: 94%\n",
      "\t ˈ m iː d s n\n",
      "\t ˈ w s w l ʃ ŋ z n ŋ\n",
      "\t ˈ l t m n ŋ\n",
      "\t l s d ˌ p iː ə ɝː l i\n",
      "\t ɑː ˈ ɑː ˌ ɛ s ə n\n",
      "Epoch 88: train loss: 0.7126\tdev loss: 1.0364                                                                                               \n",
      "\tGenerated: in train: 3%, assess: 0%, novel: 97%\n",
      "\t p ˈ eɪ r t n\n",
      "\t ˈ d iː s r\n",
      "\t ŋ ˈ b iː ɪ n\n",
      "\t l ˈ m ˌ ɪ d d\n",
      "\t l ˈ ɪ n n z t ˌ p l z\n",
      "Epoch 89: train loss: 0.7123\tdev loss: 1.0359                                                                                               \n",
      "\tGenerated: in train: 3%, assess: 0%, novel: 97%\n",
      "\t m n ˈ ɛ ə ə n z\n",
      "\t ˈ z n r s ɝ\n",
      "\t ˈ j d ð ɪ\n",
      "\t ˈ d r k\n",
      "\t ˈ ɑː ə n ɪ f ˌ b f m r z\n",
      "Epoch 90: train loss: 0.7117\tdev loss: 1.0350                                                                                               \n",
      "\tGenerated: in train: 0%, assess: 0%, novel: 100%\n",
      "\t ˈ h s k z ɑː n s l\n",
      "\t ˈ ɪ t ɝ n ɝ\n",
      "\t ˈ r ɑː ˌ p oʊ ɝː n\n",
      "\t ə ˈ ɝː uː\n",
      "\t v m ˈ m iː s t æ s s d i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91: train loss: 0.7115\tdev loss: 1.0349                                                                                               \n",
      "\tGenerated: in train: 2%, assess: 0%, novel: 98%\n",
      "\t ɪ ə ˈ g d n\n",
      "\t s ˈ z aɪ ə ə ə r\n",
      "\t ˈ iː z s ə s iː i\n",
      "\t ˈ s g ɑː\n",
      "\t ˈ l ə ɪ b k ʃ t ə s z ɪ ɛ ɪ ɑː t ɝ n\n",
      "Epoch 92: train loss: 0.7111\tdev loss: 1.0343                                                                                               \n",
      "\tGenerated: in train: 3%, assess: 0%, novel: 97%\n",
      "\t ˈ t ə t n\n",
      "\t ˈ g l tʃ t ŋ\n",
      "\t ˈ aɪ ɪ p ə n\n",
      "\t m m k\n",
      "\t ˌ æ ˈ k z dʒ ɪ z v\n",
      "Epoch 93: train loss: 0.7107\tdev loss: 1.0338                                                                                               \n",
      "\tGenerated: in train: 3%, assess: 0%, novel: 97%\n",
      "\t ˈ aɪ l l\n",
      "\t ˈ g dʒ\n",
      "\t ˈ k ɑː ˌ p d t\n",
      "\t ˈ ɝ z t k ɪ ˌ l oʊ z ɪ r l\n",
      "\t k l ˌ s t ˈ f ʃ n ɪ m\n",
      "Epoch 94: train loss: 0.7104\tdev loss: 1.0332                                                                                               \n",
      "\tGenerated: in train: 1%, assess: 0%, novel: 99%\n",
      "\t ˌ ɑː ˈ d æ s ə ŋ l ə n\n",
      "\t j k ˈ f ɔ z ɝ\n",
      "\t ˈ æ ə l eɪ k t p\n",
      "\t ˈ æ n ˌ ʌ ɝ ˌ s iː ɛ ʃ s ɝ\n",
      "\t ˈ k ˌ ə p iː v n\n",
      "Epoch 95: train loss: 0.7100\tdev loss: 1.0327                                                                                               \n",
      "\tGenerated: in train: 1%, assess: 0%, novel: 99%\n",
      "\t ˈ g oʊ ɪ tʃ\n",
      "\t ˈ r æ m d s\n",
      "\t ɝ ˈ p ɔ l ə ɝ\n",
      "\t ˈ ɝ b iː r n\n",
      "\t ˈ ə k ɝ v ə\n",
      "Epoch 96: train loss: 0.7097\tdev loss: 1.0323                                                                                               \n",
      "\tGenerated: in train: 1%, assess: 0%, novel: 99%\n",
      "\t ʃ r ɝː r ˈ t r ɪ t l\n",
      "\t j p r tʃ ɔ b ʃ θ t k ˌ l l eɪ k n\n",
      "\t ə ɛ ˈ z ˌ r l z r ə b l\n",
      "\t n ˈ l æ ə t t\n",
      "\t v s w h r r p ɑː b d\n",
      "Epoch 97: train loss: 0.7098\tdev loss: 1.0324                                                                                               \n",
      "\tGenerated: in train: 1%, assess: 1%, novel: 98%\n",
      "\t ˈ m aɪ b\n",
      "\t ˈ j ɔ r ɔɪ\n",
      "\t ˌ r w ˈ l ʌ w ə\n",
      "\t ˈ b ɪ ə l n\n",
      "\t ˈ ʒ l aɪ s tʃ t\n",
      "Epoch 98: train loss: 0.7093\tdev loss: 1.0317                                                                                               \n",
      "\tGenerated: in train: 2%, assess: 1%, novel: 97%\n",
      "\t m k ə b ˈ ɪ ə r\n",
      "\t ˈ b ɛ t n d\n",
      "\t ˈ r z s r t ŋ\n",
      "\t tʃ ˈ l oʊ ɔ t n\n",
      "\t ˈ h æ l p b ŋ l ʃ ə ə r\n",
      "Epoch 99: train loss: 0.7090\tdev loss: 1.0313                                                                                               \n",
      "\tGenerated: in train: 2%, assess: 0%, novel: 98%\n",
      "\t ˈ l oʊ r ə ɪ ˌ ɝ eɪ\n",
      "\t ˈ k uː ɪ m s ˌ r ˌ t ɝ\n",
      "\t ˈ k ɪ aʊ n\n",
      "\t ˈ m ˈ\n",
      "\t ˈ s r ə aɪ\n",
      "Epoch 100: train loss: 0.7087\tdev loss: 1.0310                                                                                               \n",
      "\tGenerated: in train: 3%, assess: 0%, novel: 97%\n",
      "\t l p ˈ g s b h ɔ j ŋ ɑː n\n",
      "\t ˈ aɪ m ɝ\n",
      "\t oʊ ʃ ˈ k s p ɪ tʃ\n",
      "\t r ˈ b w t d m ɪ\n",
      "\t t ˈ ɔ ɔ v s ɪ ˌ ə\n",
      "Epoch 101: train loss: 0.7088\tdev loss: 1.0312                                                                                               \n",
      "\tGenerated: in train: 2%, assess: 0%, novel: 98%\n",
      "\t n ˈ ʌ oʊ n ə æ p s\n",
      "\t ˈ n ə n\n",
      "\t ˈ p iː d p i d ə ɪ b ˌ oʊ s\n",
      "\t ˈ ə l ɛ d z p s ə n z z\n",
      "\t ˈ r l ɛ s r\n",
      "Epoch 102: train loss: 0.7082\tdev loss: 1.0303                                                                                               \n",
      "\tGenerated: in train: 3%, assess: 0%, novel: 97%\n",
      "\t m ˈ l\n",
      "\t ɑː ˈ g p eɪ\n",
      "\t s ˈ r s n s\n",
      "\t k ˈ l\n",
      "\t ˈ ɑː t r n z l iː\n",
      "Epoch 103: train loss: 0.7079\tdev loss: 1.0298                                                                                               \n",
      "\tGenerated: in train: 2%, assess: 0%, novel: 98%\n",
      "\t aɪ ˈ ɪ ɪ ŋ ɪ l l ɝ\n",
      "\t ˈ v l eɪ i\n",
      "\t ˌ ɑː g ˈ ɝː ɪ l\n",
      "\t ˈ ɪ t t t n\n",
      "\t ˈ ɛ ɔ ʒ b b s\n",
      "Epoch 104: train loss: 0.7078\tdev loss: 1.0298                                                                                               \n",
      "\tGenerated: in train: 2%, assess: 0%, novel: 98%\n",
      "\t ˈ g p ɔ h s r ɪ\n",
      "\t ˈ t ə p s t t\n",
      "\t k ɑː ˈ ɝː ɑː n\n",
      "\t l ˈ ʃ æ t n\n",
      "\t ˈ ɑː m b ˈ ɑː t\n",
      "Epoch 105: train loss: 0.7074\tdev loss: 1.0292                                                                                               \n",
      "\tGenerated: in train: 1%, assess: 1%, novel: 98%\n",
      "\t ˌ ɑː p n m ˈ ɛ t n ɪ\n",
      "\t ˈ ɛ ə aʊ i\n",
      "\t ˈ aɪ n v ə n\n",
      "\t ˈ eɪ ɝ dʒ l\n",
      "\t ˈ æ r r t\n",
      "Epoch 106: train loss: 0.7071\tdev loss: 1.0287                                                                                               \n",
      "\tGenerated: in train: 0%, assess: 0%, novel: 100%\n",
      "\t ˈ oʊ f b r s aɪ l\n",
      "\t ˈ n æ v iː ə eɪ d z ə ɪ ɑː ˌ ɪ ɝ oʊ iː k n ə ə\n",
      "\t ˈ dʒ aɪ ɪ d t z s\n",
      "\t ˈ eɪ n d z k\n",
      "\t ˈ ɪ ə ɝ s d n\n",
      "Epoch 107: train loss: 0.7065\tdev loss: 1.0279                                                                                               \n",
      "\tGenerated: in train: 3%, assess: 0%, novel: 97%\n",
      "\t ə ˈ p\n",
      "\t ˈ dʒ ə n ˌ iː eɪ m z t s\n",
      "\t ˈ s n s ə n\n",
      "\t ə i\n",
      "\t ˈ r\n",
      "Epoch 108: train loss: 0.7065\tdev loss: 1.0279                                                                                               \n",
      "\tGenerated: in train: 2%, assess: 0%, novel: 98%\n",
      "\t ˈ oʊ iː k\n",
      "\t ˌ iː d ˈ r ɛ n\n",
      "\t ˈ ɛ d ɔ aʊ s\n",
      "\t ˈ ˈ n ə ə v\n",
      "\t ˈ oʊ k æ ɪ b ə t aɪ\n",
      "Epoch 109: train loss: 0.7064\tdev loss: 1.0278                                                                                               \n",
      "\tGenerated: in train: 4%, assess: 1%, novel: 95%\n",
      "\t ə b ˈ ɔ aɪ ɪ ɪ ɝ\n",
      "\t ˈ b s r\n",
      "\t p ˌ ˈ ˌ ɪ n ŋ\n",
      "\t n ˈ n k t r\n",
      "\t r ˈ ŋ θ m ə n ˌ h eɪ ɪ\n",
      "Epoch 110: train loss: 0.7060\tdev loss: 1.0273                                                                                               \n",
      "\tGenerated: in train: 2%, assess: 0%, novel: 98%\n",
      "\t ˈ m v l\n",
      "\t ˈ g k uː m æ ɔ g t ə p d z\n",
      "\t ˈ g\n",
      "\t ˈ w m aɪ s n\n",
      "\t ˈ b m iː d ŋ ˌ æ b n\n",
      "Epoch 111: train loss: 0.7055\tdev loss: 1.0268                                                                                               \n",
      "\tGenerated: in train: 1%, assess: 1%, novel: 98%\n",
      "\t s p k ˈ ʌ k ɝ\n",
      "\t ˈ ɑː ˌ l θ oʊ dʒ t n\n",
      "\t ˈ s p t ə ʃ t z\n",
      "\t ɪ ˈ s s ɛ r t\n",
      "\t n h ˈ æ iː eɪ ə ə\n",
      "Epoch 112: train loss: 0.7055\tdev loss: 1.0269                                                                                               \n",
      "\tGenerated: in train: 4%, assess: 0%, novel: 96%\n",
      "\t ˌ h k iː ɪ ˈ f m\n",
      "\t ɝ ˈ ɛ tʃ\n",
      "\t n ə ˈ ɑː k d\n",
      "\t ˈ d t ɪ k v\n",
      "\t b ˈ t ɑː d ɝ\n",
      "Epoch 113: train loss: 0.7052\tdev loss: 1.0264                                                                                               \n",
      "\tGenerated: in train: 1%, assess: 0%, novel: 99%\n",
      "\t æ l ˈ s f s ɪ s n\n",
      "\t k r ˈ ʌ n ɝ\n",
      "\t t ə ˈ ʃ ʌ r m ɝ i z ɝ\n",
      "\t ˈ tʃ ə t\n",
      "\t n ˈ æ ɪ ɪ s k\n",
      "Epoch 114: train loss: 0.7050\tdev loss: 1.0260                                                                                               \n",
      "\tGenerated: in train: 3%, assess: 0%, novel: 97%\n",
      "\t ˈ v ɔ ɑː ɑː l aʊ eɪ t ə ɝ\n",
      "\t ˈ n ʌ ˌ n k ɛ ʃ\n",
      "\t ɑː ˈ ɔ aɪ aɪ t\n",
      "\t r n ˈ eɪ ˌ aɪ\n",
      "\t ˈ ɝː æ ˌ n w k ɪ\n",
      "Epoch 115: train loss: 0.7048\tdev loss: 1.0258                                                                                               \n",
      "\tGenerated: in train: 1%, assess: 0%, novel: 99%\n",
      "\t r ˌ f θ r n r ʃ m l ɑː ə ˈ iː ə z z ʃ w æ t\n",
      "\t ˈ n b l l i\n",
      "\t ɔ ˈ ɪ eɪ g ə ə\n",
      "\t ˈ iː l eɪ ə b k d\n",
      "\t ˈ s k eɪ æ ə ə v ə\n",
      "Epoch 116: train loss: 0.7048\tdev loss: 1.0258                                                                                               \n",
      "\tGenerated: in train: 0%, assess: 0%, novel: 100%\n",
      "\t ˈ iː v n r\n",
      "\t ˈ b l\n",
      "\t ˈ d m p\n",
      "\t ˈ k ɑː k k m r\n",
      "\t ˈ ɛ n m s z ɛ oʊ g\n",
      "Epoch 117: train loss: 0.7048\tdev loss: 1.0259                                                                                               \n",
      "\tGenerated: in train: 3%, assess: 0%, novel: 97%\n",
      "\t ˈ oʊ ɛ r n\n",
      "\t ˈ ɔ p l b b w ɪ\n",
      "\t ˈ r s s oʊ z ɝ\n",
      "\t ˈ ˌ ɑː m i\n",
      "\t ˈ f r d m l\n",
      "Epoch 118: train loss: 0.7044\tdev loss: 1.0253                                                                                               \n",
      "\tGenerated: in train: 4%, assess: 0%, novel: 96%\n",
      "\t ˈ oʊ tʃ n aɪ n\n",
      "\t ˈ iː d n\n",
      "\t l v ˈ w g eɪ\n",
      "\t ˈ ɔ h ʃ m k uː\n",
      "\t k h ˈ k t z\n",
      "Epoch 119: train loss: 0.7043\tdev loss: 1.0251                                                                                               \n",
      "\tGenerated: in train: 3%, assess: 0%, novel: 97%\n",
      "\t ˈ æ l ʃ ɑː\n",
      "\t ˈ v m k s\n",
      "\t ˈ ɛ l t r ə\n",
      "\t m n k ɛ k æ b r l n\n",
      "\t ˈ t s ˌ\n",
      "Epoch 120: train loss: 0.7044\tdev loss: 1.0253                                                                                               \n",
      "\tGenerated: in train: 6%, assess: 0%, novel: 94%\n",
      "\t ˈ l k æ\n",
      "\t ˈ eɪ w oʊ n\n",
      "\t ɑː ˈ uː r eɪ r ɝː\n",
      "\t ˈ æ ɪ p z s ə\n",
      "\t r ˈ r h uː æ r n l ɝ\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 121: train loss: 0.7028\tdev loss: 1.0230                                                                                               \n",
      "\tGenerated: in train: 6%, assess: 0%, novel: 94%\n",
      "\t d m ŋ ˈ ɔ ə n r s\n",
      "\t n ˈ ɑː s æ k\n",
      "\t ˈ oʊ ɝː dʒ\n",
      "\t p d ˈ b ŋ t dʒ\n",
      "\t ˈ k ɛ r\n",
      "Epoch 122: train loss: 0.7014\tdev loss: 1.0210                                                                                               \n",
      "\tGenerated: in train: 1%, assess: 1%, novel: 98%\n",
      "\t ˈ ɑː b ɪ n ɝ\n",
      "\t ˈ iː ɝ θ ə ˌ z oʊ t r r g i\n",
      "\t ˈ ˌ\n",
      "\t ˈ l ə ˌ ð l ɑː\n",
      "\t l ˈ ɛ b t eɪ r n\n",
      "Epoch 123: train loss: 0.7005\tdev loss: 1.0197                                                                                               \n",
      "\tGenerated: in train: 2%, assess: 0%, novel: 98%\n",
      "\t f ˈ v s oʊ m l\n",
      "\t ˈ iː l m ɪ ɪ oʊ i\n",
      "\t ˈ oʊ ɛ s ɝ ɪ ˌ t ɝ z ɝ z\n",
      "\t ˈ m eɪ s ə n\n",
      "\t ˈ ɑː s r oʊ\n",
      "Epoch 124: train loss: 0.6999\tdev loss: 1.0187                                                                                               \n",
      "\tGenerated: in train: 2%, assess: 0%, novel: 98%\n",
      "\t ˈ ɑː r ˌ iː l s\n",
      "\t ˈ dʒ ɔ ɪ ə\n",
      "\t ˈ m ɪ\n",
      "\t ˈ k t k t i\n",
      "\t ˈ r s m l t\n",
      "Epoch 125: train loss: 0.6995\tdev loss: 1.0182                                                                                               \n",
      "\tGenerated: in train: 1%, assess: 1%, novel: 98%\n",
      "\t ɪ ˈ ɛ r æ b ə d s\n",
      "\t r ˈ m t ɝ\n",
      "\t n n r g ˈ b ɑː i ˌ s r r\n",
      "\t ˈ ɔ ɪ oʊ l t s d k ə s\n",
      "\t ˈ s æ s d\n",
      "Epoch 126: train loss: 0.6990\tdev loss: 1.0175                                                                                               \n",
      "\tGenerated: in train: 5%, assess: 0%, novel: 95%\n",
      "\t ˈ m w ɛ\n",
      "\t ɪ k ˈ d dʒ\n",
      "\t ˈ s m aʊ i\n",
      "\t m s ˈ v g t k\n",
      "\t k ˈ ɑː ɝ n\n",
      "Epoch 127: train loss: 0.6991\tdev loss: 1.0176                                                                                               \n",
      "\tGenerated: in train: 1%, assess: 0%, novel: 99%\n",
      "\t ˈ s l d t\n",
      "\t r ˈ k k z ɛ ŋ ə k\n",
      "\t ˈ ʒ f aɪ ɝ\n",
      "\t ˈ r g r r l ɑː ˌ eɪ ə ˌ b s z b k ɔ\n",
      "\t ˈ ɛ ˌ ɪ s ˌ v aɪ\n",
      "Epoch 128: train loss: 0.6985\tdev loss: 1.0167                                                                                               \n",
      "\tGenerated: in train: 3%, assess: 0%, novel: 97%\n",
      "\t b ʃ ˈ ɑː ˌ ɑː ˌ n n\n",
      "\t ˈ oʊ ŋ r\n",
      "\t ˈ ɪ r\n",
      "\t ˈ w ə n r æ eɪ dʒ d\n",
      "\t ˈ w f eɪ ɛ f ə ˌ b k\n",
      "Epoch 129: train loss: 0.6984\tdev loss: 1.0166                                                                                               \n",
      "\tGenerated: in train: 1%, assess: 0%, novel: 99%\n",
      "\t l ˌ h h p h ɪ ˈ f iː ʃ z z ɪ\n",
      "\t f ˈ dʒ h j aɪ r\n",
      "\t ˈ p b eɪ ɝ ɝ\n",
      "\t ˈ t ɔɪ s s\n",
      "\t ˈ ɑː l s k ə n ə ŋ z z ɝ\n",
      "Epoch 130: train loss: 0.6981\tdev loss: 1.0162                                                                                               \n",
      "\tGenerated: in train: 1%, assess: 0%, novel: 99%\n",
      "\t ɛ ˈ æ æ ə d n ə ˌ ɪ n n ə k\n",
      "\t ˈ d ɑː ɝ ɝ\n",
      "\t r ˌ k k ˈ l z m ɑː p ə ə s\n",
      "\t ˈ r k h æ d ə ə ə d\n",
      "\t ˈ r ə r ə k s d\n",
      "Epoch 131: train loss: 0.6984\tdev loss: 1.0166                                                                                               \n",
      "\tGenerated: in train: 1%, assess: 1%, novel: 98%\n",
      "\t ˈ k æ t r s ə\n",
      "\t ˈ ɑː n s t f oʊ\n",
      "\t ˈ θ oʊ ʌ ə ˌ r t oʊ s z z ɪ r\n",
      "\t ˈ eɪ ɪ z r\n",
      "\t r ˈ s uː ɔ l uː iː\n",
      "Epoch 132: train loss: 0.6979\tdev loss: 1.0158                                                                                               \n",
      "\tGenerated: in train: 2%, assess: 0%, novel: 98%\n",
      "\t l ˈ tʃ t\n",
      "\t ˈ\n",
      "\t ˈ ɑː ɝ ˌ t ɛ\n",
      "\t n ɑː ɪ ˈ j ə ə\n",
      "\t ˈ k r s l s n\n",
      "Epoch 133: train loss: 0.6978\tdev loss: 1.0157                                                                                               \n",
      "\tGenerated: in train: 4%, assess: 1%, novel: 95%\n",
      "\t iː ˈ ɛ m m\n",
      "\t k ˈ k oʊ s tʃ s\n",
      "\t k ˈ n t b z ɑː d m ɪ\n",
      "\t ˈ w oʊ æ s m ɔ l\n",
      "\t s ˈ ɪ r l l t z t\n",
      "Epoch 134: train loss: 0.6976\tdev loss: 1.0154                                                                                               \n",
      "\tGenerated: in train: 6%, assess: 0%, novel: 94%\n",
      "\t ɑː d ˈ t aʊ\n",
      "\t m ˈ m b aʊ ʌ v\n",
      "\t ˌ l k l l k ɔ f g f l ɑː ə ˌ iː ŋ l\n",
      "\t n d n v ˈ ŋ p ɔɪ k r\n",
      "\t ˈ iː ə aɪ ɪ s\n",
      "Epoch 135: train loss: 0.6975\tdev loss: 1.0152                                                                                               \n",
      "\tGenerated: in train: 4%, assess: 0%, novel: 96%\n",
      "\t ˈ v m l\n",
      "\t ˌ l m m ɛ ˈ ɑː n ə ɪ n\n",
      "\t ŋ ˈ w b s t ə d\n",
      "\t ˈ dʒ ˌ ɛ k ə\n",
      "\t ˈ n æ ˈ ɝː\n",
      "Epoch 136: train loss: 0.6975\tdev loss: 1.0152                                                                                               \n",
      "\tGenerated: in train: 3%, assess: 0%, novel: 97%\n",
      "\t b ˈ d tʃ p t ɪ n\n",
      "\t f ˌ p m p ɪ ə l r m l z ˌ r v ɪ t t n\n",
      "\t k ˈ uː ɛ ɪ oʊ t l d\n",
      "\t ˈ t\n",
      "\t t s ɪ m ˈ ɑː t l i\n",
      "Epoch 137: train loss: 0.6971\tdev loss: 1.0148                                                                                               \n",
      "\tGenerated: in train: 3%, assess: 0%, novel: 97%\n",
      "\t r n ˈ r oʊ s s ˌ d oʊ t\n",
      "\t n ɝ l ˈ ɛ s i z\n",
      "\t t ˈ ʌ z ɑː r k n z r i\n",
      "\t b t r w æ ˈ ɛ θ g\n",
      "\t ˈ d ɔ d ɑː j k\n",
      "Epoch 138: train loss: 0.6970\tdev loss: 1.0146                                                                                               \n",
      "\tGenerated: in train: 4%, assess: 0%, novel: 96%\n",
      "\t ˈ w j l\n",
      "\t ˈ ɝ aɪ p ə ɝ ɛ\n",
      "\t ˈ t s l ɪ s n\n",
      "\t ˈ s ɛ g ə s ə n\n",
      "\t n ˈ t t m\n",
      "Epoch 139: train loss: 0.6974\tdev loss: 1.0151                                                                                               \n",
      "\tGenerated: in train: 3%, assess: 1%, novel: 96%\n",
      "\t ˈ l w\n",
      "\t ˈ ə ə p ɝ k ɝ\n",
      "\t ˈ l b l z\n",
      "\t ˈ n iː n\n",
      "\t ˈ p ɝ t z l ɪ ə\n",
      "Epoch 140: train loss: 0.6968\tdev loss: 1.0143                                                                                               \n",
      "\tGenerated: in train: 2%, assess: 1%, novel: 97%\n",
      "\t ˈ uː t n ɪ r ə ɝ\n",
      "\t ˈ l ɪ t ə oʊ n\n",
      "\t ˈ ɔ g ɪ z ˌ v ɔ\n",
      "\t ˈ k d s ɪ d\n",
      "\t ˈ ɔ b ə\n",
      "Epoch 141: train loss: 0.6965\tdev loss: 1.0141                                                                                               \n",
      "\tGenerated: in train: 4%, assess: 0%, novel: 96%\n",
      "\t ˈ ɔ n k\n",
      "\t ˈ æ ˌ ɝ b k\n",
      "\t ˈ æ n dʒ s\n",
      "\t ˌ h æ m ə ˈ ɑː t k t ʃ s ə t ə eɪ w\n",
      "\t ˈ iː l z oʊ oʊ l dʒ\n",
      "Epoch 142: train loss: 0.6964\tdev loss: 1.0139                                                                                               \n",
      "\tGenerated: in train: 0%, assess: 0%, novel: 100%\n",
      "\t ˈ ə ˌ t ɑː ə ˌ t z ˌ t l\n",
      "\t k ˈ k v s ɑː\n",
      "\t ˈ p b ɑː b n\n",
      "\t ˈ oʊ ə r ʃ iː\n",
      "\t ˈ t r r ɑː iː s t\n",
      "Epoch 143: train loss: 0.6962\tdev loss: 1.0137                                                                                               \n",
      "\tGenerated: in train: 3%, assess: 1%, novel: 96%\n",
      "\t ˈ r l n n\n",
      "\t ˌ h ɪ ˌ ɑː n ˈ p s i\n",
      "\t ˈ t r æ ə ə oʊ n ɪ ɪ\n",
      "\t ˈ iː l ˌ ɪ s\n",
      "\t ˈ ɔ ɪ ˌ aɪ n l n z\n",
      "Epoch 144: train loss: 0.6965\tdev loss: 1.0139                                                                                               \n",
      "\tGenerated: in train: 3%, assess: 0%, novel: 97%\n",
      "\t ˈ ɑː l z k n\n",
      "\t ˈ k s\n",
      "\t ˈ ə p s z ə t\n",
      "\t ˈ æ p n\n",
      "\t ˈ eɪ n l z\n",
      "Epoch 145: train loss: 0.6961\tdev loss: 1.0133                                                                                               \n",
      "\tGenerated: in train: 4%, assess: 0%, novel: 96%\n",
      "\t ˈ n g b ɛ i\n",
      "\t ˈ æ ˌ aɪ r ˌ ɛ iː l s\n",
      "\t ɪ ˈ æ ə ə ɑː ə ˌ g n d ə z d ə d ɪ\n",
      "\t ˈ ɪ j ʒ ˌ z ʃ θ z ə n s\n",
      "\t ˈ ˈ ʊ l z z\n",
      "Epoch 146: train loss: 0.6959\tdev loss: 1.0131                                                                                               \n",
      "\tGenerated: in train: 4%, assess: 0%, novel: 96%\n",
      "\t ˈ l s w ɛ t n r z\n",
      "\t ˈ ɛ ˌ k n ɛ ŋ r s ɝ i\n",
      "\t ˈ j ɪ s s\n",
      "\t k n l p ˌ m uː ɛ r s l r ɪ t r\n",
      "\t n ˈ h t aʊ aɪ ə z n d\n",
      "Epoch 147: train loss: 0.6959\tdev loss: 1.0131                                                                                               \n",
      "\tGenerated: in train: 4%, assess: 0%, novel: 96%\n",
      "\t ˈ ɔ r ˌ d n n\n",
      "\t ˈ w l\n",
      "\t ˈ ɑː ə r ɝ g ŋ t d dʒ\n",
      "\t n ˈ iː k n n\n",
      "\t ˈ b æ z d t ˌ l d l\n",
      "Epoch 148: train loss: 0.6958\tdev loss: 1.0130                                                                                               \n",
      "\tGenerated: in train: 0%, assess: 1%, novel: 99%\n",
      "\t l ˈ ɛ n ə l t ŋ ɪ z\n",
      "\t k ˈ æ ɝ oʊ ə n\n",
      "\t ˈ b r g g\n",
      "\t ˈ s ˌ aɪ ɪ ə t i\n",
      "\t ˈ ɛ ŋ ɪ ə s m m d\n",
      "Epoch 149: train loss: 0.6958\tdev loss: 1.0130                                                                                               \n",
      "\tGenerated: in train: 4%, assess: 0%, novel: 96%\n",
      "\t n k oʊ ˈ ə ɔ ɝ θ\n",
      "\t ˈ iː b ɪ s n\n",
      "\t s k ˌ m b ə p ɛ ɑː t aɪ ə ʃ ˌ t f r p b aɪ\n",
      "\t ˈ ɪ k l θ ə z b r oʊ\n",
      "\t ˈ b g ɑː ˌ ɛ ˌ g uː oʊ iː ɛ ə n\n",
      "Epoch 150: train loss: 0.6958\tdev loss: 1.0130                                                                                               \n",
      "\tGenerated: in train: 3%, assess: 0%, novel: 97%\n",
      "\t ˈ v ɪ t s ˌ\n",
      "\t ˈ l z ɛ ə n\n",
      "\t ˈ b t ɪ t\n",
      "\t ˈ s θ d r s s\n",
      "\t ˈ h ɑː ə ˈ æ r m ɝ t ə dʒ k\n",
      "Epoch 151: train loss: 0.6957\tdev loss: 1.0129                                                                                               \n",
      "\tGenerated: in train: 2%, assess: 0%, novel: 98%\n",
      "\t m n r f ˈ h ɛ s\n",
      "\t ˈ s g h ɛ r z ə n t l t oʊ ɝ\n",
      "\t ˈ ʌ g ə z ɪ\n",
      "\t n r ˈ r l ɛ z\n",
      "\t ˈ b s iː n\n",
      "Epoch 152: train loss: 0.6952\tdev loss: 1.0121                                                                                               \n",
      "\tGenerated: in train: 4%, assess: 0%, novel: 96%\n",
      "\t ˈ f iː v ˌ eɪ t\n",
      "\t l ˈ t ɪ s r\n",
      "\t ˌ s ɑː n ˈ l ɛ z ə\n",
      "\t ˈ m d ɑː n ɪ z l n\n",
      "\t ˈ s ɝ ə s ɪ\n",
      "Epoch 153: train loss: 0.6953\tdev loss: 1.0123                                                                                               \n",
      "\tGenerated: in train: 6%, assess: 0%, novel: 94%\n",
      "\t ˈ r n ɛ z\n",
      "\t ˈ t æ m ə\n",
      "\t ˈ ʃ s k\n",
      "\t ˈ ɪ l d ˌ f iː oʊ t z\n",
      "\t ə ˈ ɛ b iː n s\n",
      "Epoch 154: train loss: 0.6952\tdev loss: 1.0121                                                                                               \n",
      "\tGenerated: in train: 5%, assess: 1%, novel: 94%\n",
      "\t ˈ s ɪ d dʒ i ɛ\n",
      "\t ˈ l æ t b tʃ\n",
      "\t ˈ s ə\n",
      "\t ˈ r b aʊ b oʊ eɪ l i\n",
      "\t ˈ p ɛ n\n",
      "Epoch 155: train loss: 0.6954\tdev loss: 1.0124                                                                                               \n",
      "\tGenerated: in train: 3%, assess: 0%, novel: 97%\n",
      "\t r ˈ ɪ k oʊ n ə t ɑː n s\n",
      "\t ˈ ʃ d n ˌ k iː t ɝ\n",
      "\t m n ˈ ɝ n s\n",
      "\t ə ˈ l j m eɪ iː t\n",
      "\t r ˈ ɑː k iː m ɪ s\n",
      "Epoch 156: train loss: 0.6951\tdev loss: 1.0120                                                                                               \n",
      "\tGenerated: in train: 0%, assess: 0%, novel: 100%\n",
      "\t p ˈ s ɑː ˌ ɪ ə g\n",
      "\t ˈ p t s\n",
      "\t ˈ æ dʒ ˌ dʒ t v i\n",
      "\t ˈ w k ʃ\n",
      "\t p ˈ\n",
      "Epoch 157: train loss: 0.6948\tdev loss: 1.0116                                                                                               \n",
      "\tGenerated: in train: 2%, assess: 0%, novel: 98%\n",
      "\t ˈ w ɛ ˌ d h ɛ s ˌ uː p ə n\n",
      "\t ə r ˌ ɪ ˈ ʊ m p r\n",
      "\t ˈ n t b ɪ ŋ t ɑː k n\n",
      "\t ˈ ɪ l ˌ uː j ɛ ˌ r b aɪ ə ɝ d\n",
      "\t ˈ d ɝ ʃ\n",
      "Epoch 158: train loss: 0.6950\tdev loss: 1.0120                                                                                               \n",
      "\tGenerated: in train: 3%, assess: 0%, novel: 97%\n",
      "\t n ˈ æ ɪ ɑː g ə ŋ oʊ s v z m i\n",
      "\t ˈ d s eɪ ə r\n",
      "\t ˈ s ɔ s b r\n",
      "\t ˈ ə ˌ m h æ ɪ ʃ t d\n",
      "\t ˈ i\n",
      "Epoch 159: train loss: 0.6957\tdev loss: 1.0128                                                                                               \n",
      "\tGenerated: in train: 1%, assess: 0%, novel: 99%\n",
      "\t ˈ h ɛ v z\n",
      "\t n ˈ w k r l r\n",
      "\t ˈ ɪ ɪ n m t n z\n",
      "\t ˌ aɪ n ˈ m t ɑː d t\n",
      "\t ˈ eɪ k ə\n",
      "Epoch 160: train loss: 0.6947\tdev loss: 1.0114                                                                                               \n",
      "\tGenerated: in train: 2%, assess: 0%, novel: 98%\n",
      "\t ˈ l l ɑː\n",
      "\t ˈ ɔ n d z t ɪ\n",
      "\t k ˈ oʊ l æ ɝ z l ɝ\n",
      "\t ˈ ɛ z p ə r p\n",
      "\t r ˈ ɛ k ɑː k k\n",
      "Epoch 161: train loss: 0.6946\tdev loss: 1.0112                                                                                               \n",
      "\tGenerated: in train: 7%, assess: 1%, novel: 92%\n",
      "\t ˈ ɪ n s l ɝ\n",
      "\t ˈ b ɪ n\n",
      "\t t m b ˈ d m oʊ ɔ t ə n t n\n",
      "\t ˈ ʊ k ɝ k\n",
      "\t ˈ ˌ eɪ ˌ l t l\n",
      "Epoch 162: train loss: 0.6947\tdev loss: 1.0115                                                                                               \n",
      "\tGenerated: in train: 5%, assess: 0%, novel: 95%\n",
      "\t m v ˈ h k tʃ\n",
      "\t ˈ uː æ b ə p ə\n",
      "\t s n ˈ\n",
      "\t ə ə ˈ g d aɪ z l\n",
      "\t k ˈ r l g ɪ\n",
      "Epoch 163: train loss: 0.6944\tdev loss: 1.0111                                                                                               \n",
      "\tGenerated: in train: 0%, assess: 0%, novel: 100%\n",
      "\t ˈ w æ ŋ iː\n",
      "\t ˈ ʃ k æ s ɝ z ɝ\n",
      "\t k p g r ˈ ɑː ɑː m n\n",
      "\t ˈ b s eɪ ɝ t\n",
      "\t ˈ v oʊ r r ɔ t s n\n",
      "Epoch 164: train loss: 0.6944\tdev loss: 1.0111                                                                                               \n",
      "\tGenerated: in train: 2%, assess: 0%, novel: 98%\n",
      "\t ˈ n oʊ ɝ ə oʊ s ə l z\n",
      "\t ˈ k iː ŋ p ə p\n",
      "\t ˈ aɪ iː oʊ ɛ l ŋ tʃ\n",
      "\t f s ˈ æ z ə ə g n\n",
      "\t ˈ oʊ æ r oʊ s s ʃ\n",
      "Epoch 165: train loss: 0.6945\tdev loss: 1.0112                                                                                               \n",
      "\tGenerated: in train: 5%, assess: 0%, novel: 95%\n",
      "\t ˈ æ l i\n",
      "\t ˈ æ m n ɪ ɝ\n",
      "\t ˌ ɑː k s s m k n m uː\n",
      "\t d ə ˈ k s dʒ n s\n",
      "\t l ˈ h f\n",
      "Epoch 166: train loss: 0.6943\tdev loss: 1.0109                                                                                               \n",
      "\tGenerated: in train: 2%, assess: 0%, novel: 98%\n",
      "\t ˈ ɝː oʊ f r s i\n",
      "\t n ʃ ˈ r t ə ɝ\n",
      "\t ˈ ɪ k l ɪ\n",
      "\t ˈ ɛ oʊ ɪ r l\n",
      "\t ˈ h aɪ ˈ i s n\n",
      "Epoch 167: train loss: 0.6942\tdev loss: 1.0108                                                                                               \n",
      "\tGenerated: in train: 5%, assess: 0%, novel: 95%\n",
      "\t ˈ v ɛ ɝ\n",
      "\t ˈ aɪ n ə ŋ d n\n",
      "\t ˈ z f ɔ m\n",
      "\t ˈ r l k l ə r\n",
      "\t n ˈ p m ɪ ˌ æ l k\n",
      "Epoch 168: train loss: 0.6940\tdev loss: 1.0106                                                                                               \n",
      "\tGenerated: in train: 2%, assess: 0%, novel: 98%\n",
      "\t ˈ w j æ ɪ l t d\n",
      "\t ˈ k ɪ l\n",
      "\t l ˈ oʊ h b aɪ n\n",
      "\t ˈ v p iː ˌ h s ɛ\n",
      "\t ˈ ə ə\n",
      "Epoch 169: train loss: 0.6940\tdev loss: 1.0104                                                                                               \n",
      "\tGenerated: in train: 5%, assess: 0%, novel: 95%\n",
      "\t ˌ ɑː ˈ ɑː b m ŋ θ ŋ\n",
      "\t ˈ l ˌ ɔ v s s t\n",
      "\t ˈ b g ɪ ə ɪ l\n",
      "\t h p t h l r ɑː n d ˈ ɪ s ŋ ŋ ə r\n",
      "\t ˈ æ b ɝ ɪ\n",
      "Epoch 170: train loss: 0.6938\tdev loss: 1.0102                                                                                               \n",
      "\tGenerated: in train: 3%, assess: 1%, novel: 96%\n",
      "\t ˈ ʊ n ɪ s\n",
      "\t n n ˈ æ f ˌ ʃ m m ɛ\n",
      "\t v ˈ ɛ z m ə ə s dʒ ɪ ɝ\n",
      "\t ˈ æ n f ˌ iː k oʊ d\n",
      "\t ˌ h æ ˈ s ɛ b d l s\n",
      "Epoch 171: train loss: 0.6939\tdev loss: 1.0104                                                                                               \n",
      "\tGenerated: in train: 4%, assess: 1%, novel: 95%\n",
      "\t ˈ ə r d ə ə l t\n",
      "\t ˈ s r ɑː n iː ˌ f æ p dʒ\n",
      "\t ˈ p ɑː ɝ ˌ uː ə v m\n",
      "\t ˈ ɑː ɪ b θ z r z\n",
      "\t ˈ ʌ ˌ t ʌ ˌ ɝː ɪ dʒ ə ə ʃ ʃ ə oʊ l ɝ\n",
      "Epoch 172: train loss: 0.6939\tdev loss: 1.0105                                                                                               \n",
      "\tGenerated: in train: 2%, assess: 0%, novel: 98%\n",
      "\t ˈ p ɪ z ə k ɝ\n",
      "\t ˈ iː v ˌ s n d\n",
      "\t ˈ b s ɛ r d\n",
      "\t ˌ ɑː ˈ eɪ ɪ n\n",
      "\t θ l ˈ r ɑː ˌ æ n s ʃ\n",
      "Epoch 173: train loss: 0.6935\tdev loss: 1.0099                                                                                               \n",
      "\tGenerated: in train: 1%, assess: 0%, novel: 99%\n",
      "\t ˈ æ l l t ɝ\n",
      "\t n v s p k ə n ə ə ə l t ə iː\n",
      "\t k ˈ m ɔɪ ə z ə k ɑː s n\n",
      "\t n ˈ ə s d\n",
      "\t n d r ˈ ʊ m g ŋ t\n",
      "Epoch 174: train loss: 0.6936\tdev loss: 1.0099                                                                                               \n",
      "\tGenerated: in train: 1%, assess: 1%, novel: 98%\n",
      "\t l n b ˈ f v eɪ r\n",
      "\t ˈ v æ k ɔ ɪ d ɛ\n",
      "\t ə ˈ s ɪ ŋ oʊ l ɪ s n\n",
      "\t n ˈ s æ n\n",
      "\t l ˈ ə t ɪ p ʃ ɪ\n",
      "Epoch 175: train loss: 0.6932\tdev loss: 1.0094                                                                                               \n",
      "\tGenerated: in train: 4%, assess: 0%, novel: 96%\n",
      "\t ɪ ˈ t d ɝː k i\n",
      "\t r ˈ l s ə ɑː k ə l\n",
      "\t l ˌ r g b ɑː g θ b ˈ n ð ɝ\n",
      "\t ˈ s s m b n\n",
      "\t oʊ ˈ s uː ɪ ɝ i\n",
      "Epoch 176: train loss: 0.6934\tdev loss: 1.0097                                                                                               \n",
      "\tGenerated: in train: 4%, assess: 0%, novel: 96%\n",
      "\t s l p ə v ˈ l ɪ s\n",
      "\t ˈ b eɪ\n",
      "\t f ˈ h f oʊ w f r\n",
      "\t n ˌ ɔ ˌ b ɑː g s k iː ˌ f r aʊ k\n",
      "\t ʃ ˈ t m oʊ æ ʌ t s i t m oʊ ə ŋ l\n",
      "Epoch 177: train loss: 0.6931\tdev loss: 1.0093                                                                                               \n",
      "\tGenerated: in train: 4%, assess: 0%, novel: 96%\n",
      "\t ɛ ˈ ɑː ˈ ə\n",
      "\t ˈ n eɪ ə ŋ l z\n",
      "\t ˈ s j n z ɪ z ə\n",
      "\t ˈ k ɛ l\n",
      "\t ˈ r p ɪ t z iː ɪ t\n",
      "Epoch 178: train loss: 0.6930\tdev loss: 1.0091                                                                                               \n",
      "\tGenerated: in train: 2%, assess: 0%, novel: 98%\n",
      "\t t ɪ ˈ aʊ ɪ d k t l\n",
      "\t r ˈ m oʊ dʒ s l\n",
      "\t ˈ r ɛ s s ɝ\n",
      "\t ˈ aɪ t tʃ g t\n",
      "\t ˈ d k b ɛ t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 179: train loss: 0.6929\tdev loss: 1.0090                                                                                               \n",
      "\tGenerated: in train: 2%, assess: 2%, novel: 96%\n",
      "\t ˈ m d\n",
      "\t m ˈ r ɑː n ə ŋ ɝ\n",
      "\t p ə ˈ l ɪ s ˌ m ɔ aʊ oʊ ɝ\n",
      "\t ˈ d ɪ\n",
      "\t ˈ r l oʊ r b k\n",
      "Epoch 180: train loss: 0.6928\tdev loss: 1.0090                                                                                               \n",
      "\tGenerated: in train: 2%, assess: 0%, novel: 98%\n",
      "\t ˈ ɑː p n d\n",
      "\t ˈ ɪ iː ˌ ʌ m k\n",
      "\t ˈ k æ ˌ tʃ ˌ s oʊ oʊ v ɪ z\n",
      "\t ˈ ɪ n ə k s\n",
      "\t ˈ h ɑː k ˌ uː v m ɪ\n",
      "Epoch 181: train loss: 0.6927\tdev loss: 1.0086                                                                                               \n",
      "\tGenerated: in train: 4%, assess: 1%, novel: 95%\n",
      "\t s ˈ iː k d t\n",
      "\t ˈ ɪ n m ə s\n",
      "\t ˈ aʊ p æ ə ɑː s ə ɪ k\n",
      "\t m ˈ k ɛ k\n",
      "\t ˈ w aɪ j\n",
      "Epoch 182: train loss: 0.6927\tdev loss: 1.0087                                                                                               \n",
      "\tGenerated: in train: 7%, assess: 0%, novel: 93%\n",
      "\t ˈ b ɛ ˌ n s\n",
      "\t d p b b ˌ r r n\n",
      "\t ˈ m w ɪ l d\n",
      "\t ˈ s l p ɑː r ˌ m t iː\n",
      "\t r t g æ ˈ ɛ k n\n",
      "Epoch 183: train loss: 0.6924\tdev loss: 1.0083                                                                                               \n",
      "\tGenerated: in train: 3%, assess: 1%, novel: 96%\n",
      "\t ˈ ə ˌ ɑː ˈ ɑː k eɪ n ˌ oʊ w g k\n",
      "\t ˈ l ɛ ŋ i\n",
      "\t ˈ h m ɛ ə ə ə\n",
      "\t ˈ uː l v iː p\n",
      "\t ˈ ɔɪ ə ɪ v ə\n",
      "Epoch 184: train loss: 0.6923\tdev loss: 1.0081                                                                                               \n",
      "\tGenerated: in train: 6%, assess: 0%, novel: 94%\n",
      "\t ˈ eɪ ə ə ə ˈ ɪ\n",
      "\t ˈ ɛ ɔ k tʃ\n",
      "\t ˈ ɔ ə z l l ə dʒ\n",
      "\t ˈ iː t ɪ ə n r\n",
      "\t ˈ r n m s z\n",
      "Epoch 185: train loss: 0.6922\tdev loss: 1.0080                                                                                               \n",
      "\tGenerated: in train: 4%, assess: 0%, novel: 96%\n",
      "\t ʃ m d ˌ g eɪ ˌ iː ə z ˌ ɑː n l l\n",
      "\t r ˈ oʊ ɪ ˌ n r uː ɑː r ə ə v t\n",
      "\t k n ɛ ˈ n tʃ ə m ə k z t aʊ æ n n iː\n",
      "\t ˈ ɑː n m ɪ uː\n",
      "\t ˈ ɛ k s s\n",
      "Epoch 186: train loss: 0.6917\tdev loss: 1.0072                                                                                               \n",
      "\tGenerated: in train: 3%, assess: 0%, novel: 97%\n",
      "\t ˈ ɛ d b n i\n",
      "\t f n ˈ ɛ k m\n",
      "\t ə ˈ w m\n",
      "\t g ˈ s h ɪ r iː t\n",
      "\t ˈ t eɪ s ə ˌ r dʒ\n",
      "Epoch 187: train loss: 0.6918\tdev loss: 1.0074                                                                                               \n",
      "\tGenerated: in train: 3%, assess: 0%, novel: 97%\n",
      "\t ə ˈ l oʊ p aɪ i\n",
      "\t ˈ l ɛ l\n",
      "\t ˈ uː ɛ r n ɪ m s z n s\n",
      "\t b dʒ ə n ɝ ˈ ɑː l d\n",
      "\t ˈ s s d ɑː l i\n",
      "Epoch 188: train loss: 0.6914\tdev loss: 1.0069                                                                                               \n",
      "\tGenerated: in train: 1%, assess: 0%, novel: 99%\n",
      "\t ˈ ɝː b aɪ\n",
      "\t ˈ ɝː w ɛ k uː t ɪ\n",
      "\t ˈ ʊ n n ˌ ʃ l ə r ˌ iː p ə ɛ z l s\n",
      "\t n r ˈ w t z\n",
      "\t ˈ ɝ ˈ t i\n",
      "Epoch 189: train loss: 0.6910\tdev loss: 1.0063                                                                                               \n",
      "\tGenerated: in train: 3%, assess: 0%, novel: 97%\n",
      "\t ˈ f t æ t\n",
      "\t ˈ ɛ ˌ ɑː ə ˈ oʊ iː n ə d i\n",
      "\t ˈ t k uː s ɪ ɝ z\n",
      "\t ˈ l ɝ ə z n\n",
      "\t ˈ s ɪ l n i\n",
      "Epoch 190: train loss: 0.6907\tdev loss: 1.0059                                                                                               \n",
      "\tGenerated: in train: 3%, assess: 0%, novel: 97%\n",
      "\t ˈ uː oʊ ɑː ɑː d ɪ\n",
      "\t ˈ ɔ ə g r ɝ\n",
      "\t ˈ s r r ə ɪ s\n",
      "\t ˈ ɝː w æ d k ŋ ɪ\n",
      "\t ɪ ˈ g ɑː l l\n",
      "Epoch 191: train loss: 0.6906\tdev loss: 1.0057                                                                                               \n",
      "\tGenerated: in train: 6%, assess: 0%, novel: 94%\n",
      "\t n s ˈ j h oʊ aɪ s n i\n",
      "\t m ə j l n t ɪ l s ˈ eɪ ə\n",
      "\t ˈ m w ɑː r ŋ ɝ\n",
      "\t ˈ t v dʒ ɪ\n",
      "\t ˈ p eɪ k\n",
      "Epoch 192: train loss: 0.6906\tdev loss: 1.0057                                                                                               \n",
      "\tGenerated: in train: 4%, assess: 0%, novel: 96%\n",
      "\t ˈ ɛ t æ ə l g s ɝ\n",
      "\t ˈ p r s t\n",
      "\t ˈ oʊ h ɛ ˌ p j ʃ l\n",
      "\t ˈ t ɛ n ŋ l\n",
      "\t n ˈ l ɑː s ɪ d ə n s æ n\n",
      "Epoch 193: train loss: 0.6904\tdev loss: 1.0055                                                                                               \n",
      "\tGenerated: in train: 3%, assess: 0%, novel: 97%\n",
      "\t ˌ ˈ f f d g i i z i z\n",
      "\t ˈ m ˌ ɪ n t ɝ\n",
      "\t ˈ ɪ v t d\n",
      "\t ˈ k æ j n s r ə\n",
      "\t ˈ h ɛ ˌ ɪ ˌ f oʊ æ s ə ə p uː s\n",
      "Epoch 194: train loss: 0.6903\tdev loss: 1.0054                                                                                               \n",
      "\tGenerated: in train: 4%, assess: 1%, novel: 95%\n",
      "\t ˈ ɛ ɪ k p k\n",
      "\t l ˈ v t ə t s\n",
      "\t m ˈ ə k\n",
      "\t h g s ˈ ɛ l oʊ iː k tʃ r ə ə t ɪ b t z\n",
      "\t ˈ m ɪ n n s\n",
      "Epoch 195: train loss: 0.6902\tdev loss: 1.0053                                                                                               \n",
      "\tGenerated: in train: 7%, assess: 0%, novel: 93%\n",
      "\t ˈ n aɪ t uː s ə ə t s\n",
      "\t ˈ æ ˈ æ n n n\n",
      "\t s ˈ s ə l\n",
      "\t ˈ uː ɔ d ɝ n i v l k\n",
      "\t ˈ oʊ t t ɪ\n",
      "Epoch 196: train loss: 0.6901\tdev loss: 1.0049                                                                                               \n",
      "\tGenerated: in train: 4%, assess: 0%, novel: 96%\n",
      "\t g θ ˈ ɑː ˈ l ə ˌ ə\n",
      "\t ˈ n aɪ t\n",
      "\t g ˈ ɛ n m oʊ\n",
      "\t r n r ˈ n ɪ\n",
      "\t k ˈ s uː ə l b n ə l d\n",
      "Epoch 197: train loss: 0.6900\tdev loss: 1.0048                                                                                               \n",
      "\tGenerated: in train: 3%, assess: 1%, novel: 96%\n",
      "\t ˈ ɪ n r k\n",
      "\t ˈ l t ɛ l\n",
      "\t ˈ dʒ ə s ə l oʊ ə ˌ ɛ tʃ s ə t\n",
      "\t n ˌ s f b r ɔ r i\n",
      "\t ˈ t l oʊ ɑː ŋ l ɪ ɝ i t ɪ ə l\n",
      "Epoch 198: train loss: 0.6899\tdev loss: 1.0046                                                                                               \n",
      "\tGenerated: in train: 2%, assess: 0%, novel: 98%\n",
      "\t m b ɪ ə ˈ l ɛ\n",
      "\t r s m ˈ aɪ n\n",
      "\t w ˈ uː l ə\n",
      "\t g ˈ d æ n\n",
      "\t ˈ r iː n z ð\n",
      "Epoch 199: train loss: 0.6897\tdev loss: 1.0044                                                                                               \n",
      "\tGenerated: in train: 1%, assess: 0%, novel: 99%\n",
      "\t ˈ s eɪ ɝ p\n",
      "\t ˈ h m ʌ d g d\n",
      "\t ˈ ɑː ʃ d z ɪ ʃ ɝ i t ə\n",
      "\t ˈ w eɪ ɪ m t t ɪ\n",
      "\t ˈ r dʒ s iː\n",
      "Epoch 200: train loss: 0.6896\tdev loss: 1.0044                                                                                               \n",
      "\tGenerated: in train: 4%, assess: 0%, novel: 96%\n",
      "\t ˈ ɑː s ɝ s\n",
      "\t r b l z k r ɛ ˈ s\n",
      "\t r n ˈ s ɔ n ə ə r ə ˌ ɝ r\n",
      "\t s ˌ ɝː r æ ˈ w ɑː l i ɪ\n",
      "\t ˈ ʊ ʃ ɪ l n d\n",
      "Epoch 201: train loss: 0.6894\tdev loss: 1.0040                                                                                               \n",
      "\tGenerated: in train: 2%, assess: 0%, novel: 98%\n",
      "\t m b t r t b p æ ə ŋ d ə m\n",
      "\t b ˈ ɔ v n t ɪ\n",
      "\t t k ɔ ˈ n ɛ\n",
      "\t ˈ b r ɔ f ɝ\n",
      "\t ˈ b z ɑː ˌ g f s m\n",
      "Epoch 202: train loss: 0.6894\tdev loss: 1.0039                                                                                               \n",
      "\tGenerated: in train: 3%, assess: 0%, novel: 97%\n",
      "\t d l ˌ iː ˈ oʊ w uː\n",
      "\t n ˈ oʊ l l v n\n",
      "\t ˈ ɪ ɪ n p ɪ ə l\n",
      "\t ˈ s h m ɑː ə f t ˌ ɑː d ɪ k ʃ\n",
      "\t ə ˈ ɑː ˌ t ɑː tʃ ə d ɝ\n",
      "Epoch 203: train loss: 0.6894\tdev loss: 1.0039                                                                                               \n",
      "\tGenerated: in train: 4%, assess: 0%, novel: 96%\n",
      "\t r ˈ uː ɑː t d ɪ ɪ\n",
      "\t ˈ h r ɝː aɪ ˌ ə l s ɝ\n",
      "\t l ˈ iː ˌ r ˌ ɑː iː g m ɑː m d i\n",
      "\t ˈ d θ s æ m r\n",
      "\t ˈ m n ɪ ɝ\n",
      "Epoch 204: train loss: 0.6891\tdev loss: 1.0034                                                                                               \n",
      "\tGenerated: in train: 3%, assess: 2%, novel: 95%\n",
      "\t ˈ s b s t n ə m b ɝ\n",
      "\t ˈ ɪ t\n",
      "\t h s k ɪ dʒ ɪ ɪ ə ˈ eɪ l ə n\n",
      "\t ˌ ɛ ˈ ɛ g n\n",
      "\t ˈ g f ɪ k dʒ\n",
      "Epoch 205: train loss: 0.6890\tdev loss: 1.0033                                                                                               \n",
      "\tGenerated: in train: 4%, assess: 0%, novel: 96%\n",
      "\t r r n ˈ t b ə iː d ə k i eɪ t ɛ ɪ\n",
      "\t ˈ t ɝː ɛ p\n",
      "\t k ˈ l s t t\n",
      "\t ˈ s uː ɑː ˌ ɪ n s ə k dʒ n\n",
      "\t ˈ ˈ iː ə m\n",
      "Epoch 206: train loss: 0.6889\tdev loss: 1.0033                                                                                               \n",
      "\tGenerated: in train: 1%, assess: 0%, novel: 99%\n",
      "\t s r d g dʒ n p\n",
      "\t m ˈ r ɛ l z\n",
      "\t l ɪ n ˈ s ɑː z ɪ ə t ə ˌ ɪ ˌ j k\n",
      "\t ˈ g aɪ t ə n\n",
      "\t ˈ s ˌ ɛ l n ə t æ t t ɪ tʃ ɪ i i\n",
      "Epoch 207: train loss: 0.6887\tdev loss: 1.0029                                                                                               \n",
      "\tGenerated: in train: 6%, assess: 0%, novel: 94%\n",
      "\t b ˈ b ɝː eɪ ʃ n\n",
      "\t ɝː f s θ ˈ m æ k oʊ n\n",
      "\t ˈ n s p iː i\n",
      "\t p ˈ d r l ɪ z\n",
      "\t ˈ ʌ ˌ t æ t p f s ə z n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 208: train loss: 0.6886\tdev loss: 1.0029                                                                                               \n",
      "\tGenerated: in train: 3%, assess: 0%, novel: 97%\n",
      "\t h f oʊ s p s ɝ l t i\n",
      "\t k ˈ h ɛ ə ˌ r n ə iː s ŋ ˌ ə v t b\n",
      "\t ˌ ˌ ɑː ˈ ɛ ɪ ʃ z t\n",
      "\t ˈ s dʒ g eɪ t\n",
      "\t ˈ h ɔ ə ˌ ˌ ɑː ˌ b l w æ b ə ɝ m p\n",
      "Epoch 209: train loss: 0.6885\tdev loss: 1.0026                                                                                               \n",
      "\tGenerated: in train: 3%, assess: 1%, novel: 96%\n",
      "\t l ˈ ʃ\n",
      "\t oʊ ˈ ə ˈ ə ˈ\n",
      "\t ˈ d uː l ə n\n",
      "\t ˈ ɔ ɪ m ə l v ɪ l l\n",
      "\t f k p g r r s g\n",
      "Epoch 210: train loss: 0.6885\tdev loss: 1.0026                                                                                               \n",
      "\tGenerated: in train: 2%, assess: 0%, novel: 98%\n",
      "\t ˈ iː ˌ aʊ t iː ˌ ɑː l ə ʃ\n",
      "\t ˈ s p ɑː n ə s t\n",
      "\t ˈ m æ ʃ n\n",
      "\t ˈ ə n m n z\n",
      "\t ˈ d oʊ ə s\n",
      "Epoch 211: train loss: 0.6884\tdev loss: 1.0025                                                                                               \n",
      "\tGenerated: in train: 1%, assess: 0%, novel: 99%\n",
      "\t ˈ r ɪ l s n ŋ ɪ\n",
      "\t n m ɪ ˈ aɪ\n",
      "\t aʊ t r h ʌ ˈ l p\n",
      "\t f ˈ uː ə ŋ g\n",
      "\t p ɛ k ˈ t eɪ z l z z\n",
      "Epoch 212: train loss: 0.6880\tdev loss: 1.0020                                                                                               \n",
      "\tGenerated: in train: 3%, assess: 0%, novel: 97%\n",
      "\t ˈ ə n n z ˌ ɑː s s\n",
      "\t ˈ t ɛ l l z ɝ i d\n",
      "\t ˈ p z θ ɪ l z ə ə s k\n",
      "\t ˌ v s v ɪ ə n t\n",
      "\t l k ˈ t ə t t n k\n",
      "Epoch 213: train loss: 0.6882\tdev loss: 1.0022                                                                                               \n",
      "\tGenerated: in train: 4%, assess: 1%, novel: 95%\n",
      "\t ˈ t ɔ r tʃ z\n",
      "\t ˌ w tʃ ɑː uː n ɝ ə ɝ ˌ t k\n",
      "\t p s m s ə n ˌ ɪ s ˌ p ˌ ə t\n",
      "\t ˈ d oʊ ɔɪ l\n",
      "\t h s ɪ ˈ r l oʊ z æ f m\n",
      "Epoch 214; Batch 102 of 102; loss: 0.6863                                                                                                    \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/code/sonorous/sonorous/languagemodel.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_texts, dev_texts, learning_rate, max_epochs, early_stopping_rounds, batch_size, print_every)\u001b[0m\n\u001b[1;32m    256\u001b[0m             \u001b[0mprint_status\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mprint_every\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmax_epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m             train_loss, dev_loss = self._eval_and_print(\n\u001b[0;32m--> 258\u001b[0;31m                 \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_texts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_texts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_status\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m             )\n\u001b[1;32m    260\u001b[0m             \u001b[0mtrain_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/sonorous/sonorous/languagemodel.py\u001b[0m in \u001b[0;36m_eval_and_print\u001b[0;34m(self, epoch, train_texts, train_loader, dev_texts, dev_loader, print_status)\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0mNote\u001b[0m \u001b[0mthat\u001b[0m \u001b[0;32mif\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mdev_texts\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mor\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mdev_loader\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0mthen\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msecond\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m         \"\"\"\n\u001b[0;32m--> 281\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m         \u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Epoch {epoch}: train loss: {train_loss:.4f}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/sonorous/sonorous/languagemodel.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m                 \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtotal_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/sonorous/.venv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/sonorous/.venv/lib/python3.7/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    914\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0;32m--> 916\u001b[0;31m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[1;32m    917\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/sonorous/.venv/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2019\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2020\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2021\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2023\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/sonorous/.venv/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlog_softmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1315\u001b[0m         \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'log_softmax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1317\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#temp\n",
    "\n",
    "model_parameters = ModelParams(\n",
    "    rnn_type='gru', num_layers=1, embedding_dimension=100, hidden_dimension=2,\n",
    "    max_epochs=1000, early_stopping_rounds=3\n",
    ")\n",
    "\n",
    "model = LanguageModel(vocab, model_parameters, device_name='cpu')\n",
    "\n",
    "train_losses, dev_losses = model.fit(\n",
    "    train_pronunciations.pronunciation.values.tolist(),\n",
    "    dev_pronunciations.pronunciation.values.tolist(),\n",
    "    print_every=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('s', 'l', 'l', 'k', 'l', 'g', 'dʒ', 'ˌ', 'l', 'ɛ', 't')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model_parameters = ModelParams(\n",
    "    rnn_type='gru', num_layers=3, embedding_dimension=20, hidden_dimension=20,\n",
    "    max_epochs=1000, early_stopping_rounds=3\n",
    ")\n",
    "\n",
    "model = LanguageModel(vocab, model_parameters, device_name='cuda')\n",
    "\n",
    "train_losses, dev_losses = model.fit(\n",
    "    train_pronunciations.pronunciation.values.tolist(),\n",
    "    dev_pronunciations.pronunciation.values.tolist(),\n",
    "    print_every=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model_20_20_3.pt', 'wb') as fh:\n",
    "    model.save(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model_parameters = ModelParams(\n",
    "    rnn_type='gru', num_layers=1, embedding_dimension=20, hidden_dimension=20,\n",
    "    max_epochs=1000, early_stopping_rounds=3\n",
    ")\n",
    "\n",
    "model2 = LanguageModel(vocab, model_parameters, device_name='cuda')\n",
    "\n",
    "train_losses2, dev_losses2 = model2.fit(\n",
    "    train_pronunciations.pronunciation.values.tolist(),\n",
    "    dev_pronunciations.pronunciation.values.tolist(),\n",
    "    print_every=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model_20_20_1.pt', 'wb') as fh:\n",
    "    model2.save(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model_parameters = ModelParams(\n",
    "    rnn_type='gru', num_layers=1, embedding_dimension=20, hidden_dimension=10,\n",
    "    max_epochs=1000, early_stopping_rounds=3\n",
    ")\n",
    "\n",
    "small_model = LanguageModel(vocab, model_parameters, device_name='cuda')\n",
    "\n",
    "_ = small_model.fit(\n",
    "    train_pronunciations.pronunciation.values.tolist(),\n",
    "    dev_pronunciations.pronunciation.values.tolist(),\n",
    "    print_every=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model_20_10_1.pt', 'wb') as fh:\n",
    "    small_model.save(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model_parameters = ModelParams(\n",
    "    rnn_type='gru', num_layers=1, embedding_dimension=10, hidden_dimension=10,\n",
    "    max_epochs=1000, early_stopping_rounds=3\n",
    ")\n",
    "\n",
    "model_10_10_1 = LanguageModel(vocab, model_parameters, device_name='cuda')\n",
    "\n",
    "_ = model_10_10_1.fit(\n",
    "    train_pronunciations.pronunciation.values.tolist(),\n",
    "    dev_pronunciations.pronunciation.values.tolist(),\n",
    "    print_every=1,\n",
    ")\n",
    "\n",
    "with open('model_10_10_1.pt', 'wb') as fh:\n",
    "    model_10_10_1.save(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model_parameters = ModelParams(\n",
    "    rnn_type='gru', num_layers=1, embedding_dimension=10, hidden_dimension=5,\n",
    "    max_epochs=1000, early_stopping_rounds=3\n",
    ")\n",
    "\n",
    "model_10_5_1 = LanguageModel(vocab, model_parameters, device_name='cuda')\n",
    "\n",
    "_ = model_10_5_1.fit(\n",
    "    train_pronunciations.pronunciation.values.tolist(),\n",
    "    dev_pronunciations.pronunciation.values.tolist(),\n",
    "    print_every=1,\n",
    ")\n",
    "\n",
    "with open('model_10_5_1.pt', 'wb') as fh:\n",
    "    model_10_5_1.save(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model_parameters = ModelParams(\n",
    "    rnn_type='gru', num_layers=1, embedding_dimension=10, hidden_dimension=3,\n",
    "    max_epochs=1000, early_stopping_rounds=3\n",
    ")\n",
    "\n",
    "model_10_3_1 = LanguageModel(vocab, model_parameters, device_name='cuda')\n",
    "\n",
    "_ = model_10_3_1.fit(\n",
    "    train_pronunciations.pronunciation.values.tolist(),\n",
    "    dev_pronunciations.pronunciation.values.tolist(),\n",
    "    print_every=1,\n",
    ")\n",
    "\n",
    "with open('model_10_3_1.pt', 'wb') as fh:\n",
    "    model_10_3_1.save(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model_parameters = ModelParams(\n",
    "    rnn_type='rnn', num_layers=1, embedding_dimension=10, hidden_dimension=3,\n",
    "    max_epochs=1000, early_stopping_rounds=3\n",
    ")\n",
    "\n",
    "rnn_10_3_1 = LanguageModel(vocab, model_parameters, device_name='cuda')\n",
    "\n",
    "_ = rnn_10_3_1.fit(\n",
    "    train_pronunciations.pronunciation.values.tolist(),\n",
    "    dev_pronunciations.pronunciation.values.tolist(),\n",
    "    print_every=1,\n",
    ")\n",
    "\n",
    "with open('rnn_10_3_1.pt', 'wb') as fh:\n",
    "    rnn_10_3_1.save(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "param_grid = ParameterGrid({\n",
    "    'embedding_dimension': [10, 50, 100],\n",
    "    'hidden_dimension': [10, 50, 100],\n",
    "})\n",
    "\n",
    "records = []\n",
    "for params in tqdm(param_grid):\n",
    "    model_parameters = ModelParams(rnn_type='gru', num_layers=1, max_epochs=1000, early_stopping_rounds=1, **params)\n",
    "    model = LanguageModel(vocab, model_parameters, device_name='cuda')\n",
    "\n",
    "    print('Model Params:', model_parameters)\n",
    "    \n",
    "    train_losses, dev_losses = model.fit(\n",
    "        train_pronunciations.pronunciation.values.tolist(),\n",
    "        dev_pronunciations.pronunciation.values.tolist()\n",
    "    )\n",
    "    \n",
    "    for epoch, (train_loss, dev_loss) in enumerate(zip(train_losses, dev_losses), start=1):\n",
    "        record = params.copy()\n",
    "        record['epoch'] = epoch\n",
    "        record['train_loss'] = train_loss\n",
    "        record['dev_loss'] = dev_loss\n",
    "        \n",
    "        record['rnn_type'] = 'rnn'\n",
    "        record['num_layers'] = 1 \n",
    "    \n",
    "        records.append(record)\n",
    "\n",
    "models_df = pd.DataFrame.from_records(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "param_grid = ParameterGrid({\n",
    "    'embedding_dimension': [50],\n",
    "    'hidden_dimension': [2],\n",
    "})\n",
    "\n",
    "records = []\n",
    "for params in tqdm(param_grid):\n",
    "    model_parameters = ModelParams(rnn_type='rnn', num_layers=1, max_epochs=1000, early_stopping_rounds=1, **params)\n",
    "    model = LanguageModel(vocab, model_parameters, device_name='cpu')\n",
    "\n",
    "    print('Model Params:', model_parameters)\n",
    "    \n",
    "    train_losses, dev_losses = model.fit(\n",
    "        train_pronunciations.pronunciation.values.tolist(),\n",
    "        dev_pronunciations.pronunciation.values.tolist()\n",
    "    )\n",
    "    \n",
    "    for epoch, (train_loss, dev_loss) in enumerate(zip(train_losses, dev_losses), start=1):\n",
    "        record = params.copy()\n",
    "        record['epoch'] = epoch\n",
    "        record['train_loss'] = train_loss\n",
    "        record['dev_loss'] = dev_loss\n",
    "    \n",
    "        records.append(record)\n",
    "\n",
    "models_df = pd.DataFrame.from_records(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_df = pd.DataFrame.from_records(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are {meow} models with a dev error of around .76. I'll choose the simplest one, which \n",
    "\n",
    "* point out that no matter how low the train error gets, the dev error\n",
    "* which model parameters fail to ever get to the lowest dev error\n",
    "* which model parameters overfit the most\n",
    "\n",
    "* isolate the group of models with about .76 dev error. choose the simplest one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = models_df[(models_df.embedding_dimension==50) & (models_df.hidden_dimension==100) & (models_df.num_layers==3)]\n",
    "t = t.set_index('epoch')\n",
    "t.dev_loss.plot()\n",
    "t.train_loss.plot()\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_df.sort_values('dev_loss').iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the best \"model\" was at an earlier epoch I don't have access to it. So I'll train a model with that model's parameters and set the number of epochs to MEOW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model = PhonemeLM(\n",
    "    phoneme_to_idx, device='cuda', rnn_type='gru',\n",
    "    embedding_dimension=50, hidden_dimension=100, num_layers=3,\n",
    "    max_epochs=69, early_stopping_rounds=69, batch_size=1024,\n",
    ")\n",
    "\n",
    "train_loss, dev_loss = model.fit(train_df.pronunciation.values.tolist(), dev_df.pronunciation.values.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I have what I hope is the best model I can test how well it does on the holdout test set, which I haven't look at at all during the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = build_data_loader(test_df.pronunciation.values.tolist(), lm.phoneme_to_idx)\n",
    "lm.evaluate(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_loader = build_data_loader(dev_df.pronunciation.values.tolist(), lm.phoneme_to_idx)\n",
    "lm.evaluate(dev_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: compute the test error for the final model. Bar chart for the train, dev, and test errors\n",
    "\n",
    "Comment on findings, probably that test is higher and that's expected because language models are very sensitive to corpus difference (and I probably overfit the dev set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Final Model ##\n",
    "Now that I've found the best parameters for the model according to the dev set I'll train a final model using all of the data. This should increase model performance overall since more data is better, but is also necessary since I'll be using the model to predict probabilities of all English words. If some of those words weren't in the training set they would artificially get lower probabilities. (Another approach here could be to train a model on e.g. 4/5 folds of the data and make predictions about the remaining 1/5, doing that 5 times to get unbiased predictions for all data, but this would have taken much longer to run.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model_params = ModelParams(\n",
    "    rnn_type='gru', embedding_dimension=50, hidden_dimension=50, num_layers=1,\n",
    "    max_epochs=3, early_stopping_rounds=3, batch_size=1024\n",
    ")\n",
    "\n",
    "language_model = LanguageModel(vocab, model_params, device_name='cpu')\n",
    "\n",
    "_ = language_model.fit(pronunciations.pronunciation.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, I'll save the model so I can use it in the next notebook, `Phoneme Exploration.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('phoneme_language_model.pt', 'wb') as fh:\n",
    "    model.save(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
